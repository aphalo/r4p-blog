---
title: "Linear Models (DRAFT)"
subtitle: "An introduction using `lm()`"
author: "Pedro J. Aphalo"
date: 2023-11-27
date-modified: 2023-11-27
categories: [R, model fitting]
keywords: [predicted values, residual values, parameter estimates, model formulas]
format:
  html:
    code-fold: true
    code-tools: true
abstract: 
  In this page I give a brief presentation on the mechanics of fitting statistical models to observed data using R. I use a lineal model (LM), polynomial regression, as example. I describe the most important values contained in the model fit object returned by function `lm()` and how to extract them. I rely heavily on diagrams and data plots.
---

## Introduction

_Fitting a model_ to data consists in finding the parameter values that best
explain the data or observations. These parameter values are estimates of
those in a larger population of possible observations from which we have
drawn a sample or subset. For deciding what is best, we need a criterion, and
the most frequently used one is minimizing the "residual variance" by the
method of ordinary least squares (OLS).

_Model selection_ involves comparing models that differ in their structure, i.e., 
in the formula that relates parameters and data. Here we also need a criterion
to "measure" how well a model describes the data, for example, AIC or BIC,
which are information criteria. Alternatively, we can just choose "the simplest
model that does a good enough job".

Different approaches to model fitting make different assumptions about the 
observations. What assumptions are reasonable for our data determines what 
methods are applicable. Out of these, we can choose the one that best suits
the nature of our scientific or practical question or hypothesis.

Modern statistical algorithms, and their implementation in R, strive to be
widely applicable. I will emphasize this as well as what is common to different
methods.

In general, there are many different aspects of a model fit that we may be
interested in. The most computationally intensive step is the fitting itself.
Thus R's approach is to save the results from fitting, or _fitted model_, and
separately query it for different derived numerical and graphical output as 
needed.

::: callout-tip
In this page some code chunks are "folded" so as to decrease the clutter. Above the R
output, text or plots, you will find a small triangle followed by "Code".
Clicking on the triangle "unfolds" the code chunk making visible the R code used
to produce the output shown.
The code in the chunks can be copied by clicking on the top right corner, where
an icon appears when the mouse cursor hovers over the code listing.
:::

Attach packages used for plotting.

```{r, message = FALSE}
library(ggpmisc)
library(ggbeeswarm)
library(broom)
theme_set(theme_bw(16))
```

::: callout-caution
# Anscombe's linear regression examples

This classical example from Anscombe (1973) demonstrates four very different data sets that yield exactly the same results when a linear regression model is fit to them, including $R^2$ and $P$ values.

```{r}
# we rearrange the data
my.mat <- matrix(as.matrix(anscombe), ncol=2)
my.anscombe <- 
  data.frame(x = my.mat[ , 1],
             y = my.mat[ , 2],
             case=factor(rep(1:4, rep(11,4))))
my.formula = y ~ x
ggplot(my.anscombe, aes(x,y)) +
  geom_point(shape=21, fill="orange", size=3) +
  geom_smooth(method="lm", formula = my.formula) +
  stat_poly_eq(formula = my.formula, 
               parse = TRUE, 
               label.y = "top", 
               label.x = "left", 
               use_label(c("eq", "R2", "P"))) +
  facet_wrap(~case, ncol=2) +
  theme_bw(16)
```
  1.  If these four sets of data were real observations from an experiment, would you conclude the same in all cases with respect to the effect of $x$ on the observed $y$?
  1.  In which of the four cases the estimated line best described the _overall trend_ of the response?
  1-  What is the difficulty in each of the other three cases?

These concocted data examples demonstrates that when fitting any model to data, extreme caution is always needed. **We need to check graphically the observations and how well model estimates fit them.** $R^2$, $P$ values, and other parameter's estimates _must always be considered in the light of the data, rather than in isolation_.
:::

## Linear models (LM)

I will start with some "unusual examples" (crazy approaches?) to try to convince you that there is much in common between model fitting and the simple statistics you already know. Grasping this is important as general concepts allows us to understand the bigger picture, and thus we can learn new methods as variations of, or extensions to, what we already know.

### The mean (= average) as a fitted linear model

A vector of 20 random numbers to play with.

```{r}
#| code-fold: false
my.vector <- rnorm(20)
```

```{r}
#| code-fold: false
mean(my.vector)
var(my.vector)
```
We fit a linear model, with only an intercept.

```{r}
#| code-fold: false
fm1 <- lm(my.vector ~ 1)
summary(fm1)
anova(fm1)
```

To fit the mean as a linear model, the "Mean Sq", or MS, which is just the same variance computed above was minimized.

So fitting a model is in fact similar to computing the mean of a single variable. However, while the mean is a single parameter, we can fit models with several parameters which are adjusted simultaneously to fit a point, curve, surface or hyper-surface, depending on the case.

### The _t_-test as a fitted model linear model

Two vectors of numbers to play with.

```{r}
#| code-fold: false
vec1 <- 0 + rnorm(20)
vec2 <- 1 + rnorm(20)
```

We can assemble a `data.frame` with them.

```{r}
#| code-fold: false
df1 <- data.frame(obs = c(vec1, vec2), group = rep(c("g1", "g2"), each = 20))
```

```{r}
ggplot(df1, aes(group, obs)) +
  stat_summary(fun.data = mean_se, color = "blue") +
  geom_beeswarm()
```

```{r}
#| code-fold: false
mean(vec1)
mean(vec2)
sd(vec1)
sd(vec2)
```
```{r}
#| code-fold: false
t.test(vec1, vec2, var.equal = TRUE)
```
```{r}
#| code-fold: false
t.test(obs ~ group, data = df1, var.equal = TRUE)
```
```{r}
#| code-fold: false
fm2 <- lm(obs ~ group, data = df1)
summary(fm2)
anova(fm2)
```

So, these two examples above show that model fitting provides estimates of population parameters, identically to computing a mean, as well as serving as the basis for tests of significance identical to the _t_-test. In addition, as we will see below, linear models set up more complex tests of significance, as well as fit models that require the simultaneous estimation of multiple parameters.

### ANOVA, regression and linear models

Linear models include what is called Analysis of Variance (ANOVA), linear regression, multiple regression, Analysis of Covariance, and a few other variations all wrapped together. So, one procedure that caters for a wide range of situations and designs of experiments.

Linear models can be computed very efficiently but make several assumptions about the data. Normally distributed and homogeneous variation of residuals.

## ANOVA vs. Regression

In one-way ANOVA for a completely randomized design we have the
following model: every population mean contains $\mu$, whereas the
$i$th treatment contains $\tau_i$ but no other $\tau$.

The observations $y_{ij}$ follow the linear model
$$
y_{ij} = \mu + \tau_i + \epsilon_{ij},
$$
where $\mu$ is the population mean, $\tau_i$ the effect of
treatment $i$, and $\epsilon_{ij}$ the random variation associated
with observation $y_{ij}$.

We assume that the residuals $\epsilon_{ij}$
are independent and equally distributed --- $\epsilon_{ij} \sim
N(0,\sigma^2)$, where variance $\sigma^2$ is unknown and
independent of group $i$.\vspace{2cm}

---

In linear regression the $y$ values are obtained from several
populations, each population being determined by the corresponding
$x$ value. The $y$ variable is called _dependent variable_
and the $x$ variable is called _independent variable_.

The observations $y_{ij}$ follow the linear model
$$
y_{ij} = \alpha + \beta x_i + \epsilon_{ij},
$$

where $\alpha$ is the population mean when $x=0$, $x_i$ are the
observed values for the independent variable, $\beta$ is the
coefficient describing the slope, and $\epsilon_{ij}$ the random
variation associated with observation $y_{ij}$.

We also assume that the residuals
$\epsilon_{ij}$ are independent and equally distributed ---
$\epsilon_{ij} \sim N(0,\sigma^2)$, where variance $\sigma^2$ is
unknown and independent of $x$.

It is assumed that the $x$ is measured without error.

Regression provides a means of predicting $y$ from $x$, but not the reverse.

## Linear models in R

Both **ANOVA** and **regression** are described by **linear models** and have
much more in common than what it looks at first sight.

In regression analysis _dummy_ variables can be used to code
treatments.

In R little distinction is made between ANOVA and regression.

The function`lm()` can be used for both, whether the variables
in the model are `factor`s or `numeric` vectors
determines the coding (of the matrix used for describing the tests).

The function `aov()` uses `lm()` internally but differs in
that summary gives an ANOVA table, and in that it can also deal
directly with hierarchical models (e.g. split-plot designs).

## Examples in R



### Interpretation of the intercept

We generate some artificial data (it will be different each time you run the code below.)

Why will it be different? Try it?

```{r}
x <- 100:120
y <- x * 0.02 + rnorm(21, sd = 0.05)
my.data <- data.frame(x, y)

```

The we fit a model, based on the tests in the code chunk below, will in be an ANOVA or a regression? 

```{r}
is.factor(x)
is.numeric(x)
is.factor(y)
is.numeric(y)
```

```{r}
mf1 <- lm(y ~ x, data = my.data)
anova(mf1)
summary(mf1)
plot(y ~ x, data = my.data)
abline(mf1)
```

```{r}
mf2 <- lm(y ~ I(x - 100), data = my.data)
anova(mf2)
summary(mf2)
plot(y ~ I(x - 100), data = my.data)
abline(mf2)
```

### Residuals

```{r}
plot(mf1, which = 1)
plot(mf1, which = 2)
```

In _regression_ the assumption is that the explanatory variable is **measured** without error.

What does this mean? Does it mean that the population of _x_ has no variation? or something else?

### Regression of _x_ on _y_

If we switch _X_ and _y_ we now assume that _y_s are **measured** without error.

### Regression and change points

We will use a modified dataset, based on the pH data for rivers of
the South Island of New Zealand.

First means for each month were calculated from the five
observations in each random sample.

Then a new variable was created by adding:
`c(rep(0, 60), (1:60) / 40)` to the vector
holding the mean pH values.

This is a linear trend starting at the beginning of the third year
of observations and having an increase in pH of 0.05 units per
month.

### Data used in examples

```{r}
library(lubridate)
load("data-files/SI.long.rda")
SI.data <- data.frame(date = ymd("1980-01-01", tz = "UTC") + months(1:120),
                      month = 1:120,
                      pH = south.island.long$pH)
SI.data$pH.tr <- SI.data$pH + c(rep(0, 60), (1:60) / 40)
```

```{r}
plot(pH.tr ~ month, data = SI.data, col = "red")
points(pH ~ month, data = SI.data)
```

## Linear regression

### Original data

```{r}
SI.lm <- lm(pH ~ month, data = SI.data)
anova(SI.lm )
summary(SI.lm)
```

### Data with added slope

We first try a linear regression, and do plots for diagnosis:

```{r}
SI.tr.lm <- lm(pH.tr ~ month, data=SI.data)
anova(SI.tr.lm )
summary(SI.tr.lm)
```

```{r}
plot(SI.tr.lm, which=1, pch=16)
plot(pH.tr ~ month, data=SI.data, pch=16)
lines(fitted(SI.tr.lm) ~ month,
 data=SI.data, type="l", lwd=2, lty=2)

```

The coefficient for month is very significant, but we should check
the residuals to see if the fit is good or not.

## Second degree polynomial

We now try a second degree polynomial.

```{r}
SI.tr2.lm <- lm(pH.tr ~ month + I(month^2), data=SI.data)
plot(SI.tr2.lm, which = 1, pch = 16)
plot(pH.tr ~ month, data = SI.data, pch = 16)
lines(fitted(SI.tr2.lm) ~ month,
 data=SI.data, type = "l", lwd = 2, lty = 2)
summary(SI.tr2.lm)

```

Both the linear and quadratic terms are significant.

We can also
compare the fits to the two models using `anova`:

```{r}
anova(SI.tr.lm, SI.tr2.lm)
```

## Two separate linear regressions

Because we creted the data artifically we know that there is a "change point"
half way into the time series. We can fit two separate linear models, one to
eacch half of the data, and compare them to that fitted to all data, shown above.

```{r}
SI.tr.lm <- lm(pH.tr ~ month, data=SI.data)
SI.tr.A.lm <- lm(pH.tr ~ month, data=SI.data,
 subset=month<=60)
SI.tr.B.lm <- lm(pH.tr ~ month, data=SI.data,
 subset=month>60)
```

```{r}
plot(pH.tr ~ month, data=SI.data, pch=16)
abline(SI.tr.lm, col="black", lty=2, lwd=2)
abline(SI.tr.A.lm, col="red", lty=3, lwd=2)
abline(SI.tr.B.lm, col="blue", lty=4, lwd=2)
legend(2,9,
 legend=c("All", "month <= 24", "month > 24"),
 col=c("black","red","blue"), lty=c(2,3,4), lwd=2)
title("Linear regressions")

```

### The three fitted regression models

```{r}
anova(SI.tr.lm)
```

```{r}
anova(SI.tr.A.lm)
```

```{r}
anova(SI.tr.B.lm)
```

```{r}
summary(SI.tr.B.lm)
```

In the separate fits, the estimate of the coefficient for month (slope) is now
close to that used to generate the data.

# Chow's _F_ test

With real data, we would not know the position or even whether there is
significanr a change point in the data. We can use Chow's _F_ test to test if
fitting two separate regressions according to a known change point, as just
shown, is significantly better than fitting a single regression.

The models are fitted by ordinary least squares (OLS).
An $F$-value is calculated based on the residuals. The numerator
measures how much the fit improves (how much the residuals
decrease) by fitting two separate lines. The denominator measures
the error based on the residuals for the `bigger' model (two
separate regressions).

The test proposed by Chow for a known change point can be extended
to the case of an unknown change point.

These tests are used to find a single change point in the series.
They do not test for whether there is a trend or not, they test
for a change in the 'structure' of the series.

As implemented they can be used for testing an unreplicated
series, so we use the mean of the five observations from each
month's sample.

As we just saw the procedure is to divide the data into two parts,
before and after the possible change point and to do two separate
fits, one for each part of the data, and then compare these fits
to fitting a single curve to all data.

The difference is that now we test all possible change points in a
given interval.

Consequently, the _F_-statistic is calculated for all possible
change points in an interval.

These _F_-statistic values can be plotted or used to calculate the
probability for the existence of a change point.

First we look for a change in slope or intercept, in the original
data set.

```{r}
library(strucchange)
SI.fs <- Fstats(pH ~ month, from = 0.25,
 data=SI.data)
sctest(SI.fs)

```

Then we look for a change in slope or intercept, in the data set
with a trend.

```{r}
SI.tr.fs <- Fstats(pH.tr ~ month, from = 0.25,
 data=SI.data)

 sctest(SI.tr.fs)

```

# Generalized fluctuation tests

These are based either on estimates of regression coefficients or on residuals.
Estimates from fits to all data are compared to those from subsamples of the
data. One way of choosing the subsamples is using a moving window. If there are
no structural changes the estimates from all data and from subsamples should be
similar.

If there are structural changes, the true coefficients will change over time,
and the estimates from some subsamples of the data will differ... or the
residuals will differ.

We again first test the original data.

```{r}
SI.efp <- efp(pH ~ month, type="OLS-MOSUM",
 data=SI.data)
plot(SI.efp)
sctest(SI.efp)

```

And then we test the data with a trend added.

```{r}
SI.tr.efp <- efp(pH.tr ~ month, type="OLS-MOSUM",
 data=SI.data)
plot(SI.tr.efp)
sctest(SI.tr.efp)

```

The empirical fluctuation process, in the this case the sum over a
moving window of the residuals, crosses the $\alpha = 0.05$
boundary line at about the place were the change in slope starts.

# Fitting a model with a hinge

Linear models allow a lot of flexibility. It is important to understand that
models representing the same fitted lines or curves can be fitted using
different model formulas, which result in different tests of significance. We
will go in this section through multiple examples. In all cases, one should
plot the predictions from the fitted model together with the data, as this is
a very effective way of judging the suitability of the model. This was
demonstrated by Anscombe's artificial data, but applies to any data set and
model.

## Using a factor

When there is a knwon change point in the data (as we detected with the
tests above), we need to fit a model that corresponds to a function that
can have a knot or hinge. The simplest such model is two straight lines joined
at a knot. This is the simplest possible case of what is called a _linear spline_.

The first stept is to create a factor that informs whether observations apear
before or after the change point.

```{r}
SI.data$half <- factor(ifelse(SI.data$month < 60, "first", "second"))
```

In the model fitted below `month` is a numeric variable and `half` a factor
with two levels.

```{r}
mflspl1 <- lm(pH.tr ~ month * half, data = SI.data)
summary(mflspl1)
```

## Using dummy variables

Instead of using a factor we can use dummy variables, numeric variables that take either
0 or 1 as value. With start with the equivalent of the contrasts that R used by default
in the example immediately above. 

```{r}
SI.data$dummy.early <- ifelse(SI.data$month < 60, 1, 0)
SI.data$dummy.late <- ifelse(SI.data$month < 60, 0, 1)
```

```{r}
mflspl2 <- lm(pH.tr ~  1 + month + I(1 * dummy.late) + I(month * dummy.late), data = SI.data)
summary(mflspl2)
```

We now will set up the model differently. You will need to work out what is
the difference with respect to the estimates and their significance. Note
that in this case "- 1" is needed to remove the overall intercept from the model.

```{r}
mflspl3 <- lm(pH.tr ~  I(1 * dummy.early) + I(month * dummy.early) +
                I(1 * dummy.late) + I(month * dummy.late) - 1, data = SI.data)
summary(mflspl3)
```
### Plotting the fitted values

```{r}
SI.data$pH.tr.fitted1 <- fitted(mflspl1)
SI.data$pH.tr.fitted2 <- fitted(mflspl2)
SI.data$pH.tr.fitted3 <- fitted(mflspl3)
```

```{r}
plot(pH.tr ~ month, data = SI.data)
lines(pH.tr.fitted1 ~ month, data = SI.data)
lines(pH.tr.fitted2 ~ month, data = SI.data, col = "red")
lines(pH.tr.fitted3 ~ month, data = SI.data, col = "Blue")
```
### Plotting the predicted values

As R users nowadays use package 'ggplot2' instead of R's own functions for 
plotting. In the rest of this archive I use R's `plot()` but in the next
two examples I use `ggplot()` as in this case is simpler to use.

```{r}
new.data <- data.frame(month = 1:120)
new.data$half <- factor(ifelse(new.data$month < 60, "first", "second"))
```

```{r}
library(ggplot2)
predict.data <- cbind(new.data, 
                      predict(mflspl1, new.data, interval = "confidence")) 
ggplot(data = predict.data) +
  geom_point(data = SI.data, 
             mapping = aes(x = month, y = pH.tr)) +
  geom_ribbon(alpha = 0.25, color = NA, 
              mapping = aes(x = month, y = fit, ymax = upr, ymin = lwr)) +
  geom_line(color = "red", mapping = aes(x = month, y = fit))
```

For a predition with the third model we need new data containing the dummy
variables, instead of the factor.

```{r}
new.data <- data.frame(month = 1:120)
new.data$dummy.early <- ifelse(new.data$month < 60, 1, 0)
new.data$dummy.late <- ifelse(new.data$month < 60, 0, 1)
```

```{r}
library(ggplot2)
predict.data <- cbind(new.data, predict(mflspl3, new.data, interval = "confidence")) 
ggplot(data = predict.data) +
  geom_point(data = SI.data, mapping = aes(x = month, y = pH.tr)) +
  geom_ribbon(alpha = 0.25, color = NA, mapping = aes(x = month, y = fit, ymax = upr, ymin = lwr)) +
  geom_line(color = "red", mapping = aes(x = month, y = fit))
```

We can see that even though the parametrization is different the fit and its
confidence band remain the same.

### Questions

**A)** What is the difference between the two approaches?
**B)** What do the parameter estimates represent in each case?
**C)** Although you get different estimates from the two models, can you calculate the same value for the slope in both cases?

# Robust and resistant statistics

If changing a small part of the body of data, perhaps drastically,
can change the value of the summary substantially, the summary is
not resistant. Example: the **mean**.

Conversely, if a change of a small part of the data, no matter
what part or how substantially, fails to change the summary
substantially, the summary is said to be _resistant_.
Example: the **median**.

_Robustness:_ lack of susceptibility to the effects of
non-normality.

_Robustness of validity._ example: confidence interval for
the population median based on the sign test.

_Robustness of efficiency:_ how much information is
extracted from the data under different situations.

A robust statistic performs well under different circumstances.

# Robust and resistant regression

Robust regression methods are at least partially resistant to
outliers, those called resistant methods are even more resistant.

There are several methods that can be used for robust regression.

There are also several methods for resistant regression.

In most cases they yield only approximate solutions calculated by
iteration.

We will not discuss the complexities of how they work, but give an
example of the use of weighted likelihood, a recently developed
method.

As an oversimplification it can be said that this method assigns a
weight to each observation based on the relationship between a
chosen model distribution and the empirical cumulative
distribution.

## Data with outliers

We first add six outliers to the pH data from rivers of the South
Island of New Zealand. That is to the original data rather than those data with
the added change point we fitted models to in the examples immediatley above.

```{r}
attach(SI.data)
SI.data$pH.out <- SI.data$pH
SI.data$pH.out[110:115] <- SI.data$pH[110:115] + 2.0
detach()
```


## Linear regression by OLS

**OLS** = ordinary least squares, the most usual method for model fitting.

```{r}
SI.out.lm <- lm(pH.out ~ month, data=SI.data)
summary(SI.out.lm)
```

Because of the six outliers, the slope for month is significantly
different from zero.

### Plotting the prediction

```{r}
new.data <- data.frame(month = 1:120)
```

```{r}
library(ggplot2)
predict.out.data <- cbind(new.data, 
                      predict(SI.out.lm, new.data, interval = "confidence")) 
ggplot(data = predict.out.data) +
  geom_point(data = SI.data, 
             mapping = aes(x = month, y = pH.out)) +
  geom_ribbon(alpha = 0.25, color = NA, 
              mapping = aes(x = month, y = fit, ymax = upr, ymin = lwr)) +
  geom_line(color = "red", mapping = aes(x = month, y = fit))
```

## Robust regression with wle (weighted likelihood)

WLE works by assuming a given distribution for the residuals, the Normal. Then
uses the likelihood of the residuals and downweights those in the empirical
distribution that are exceptional compared to the Normal.

```{r}
library(MASS)
SI.out.rlm <- rlm(pH.out ~ month, data=SI.data)
summary(SI.out.rlm)
plot(SI.out.rlm, ask = FALSE)
```

Now the outliers are ignored, and  the coefficient for
month does not differ significantly from zero.

## Plotting the lines

```{r}
plot(pH.out ~ month, data=SI.data, pch=16)
lines(fitted(SI.out.rlm) ~ month, data=SI.data)
lines(fitted(SI.out.lm) ~ month, data=SI.data, lty=2)
legend(x = 0, y = 9.5, legend=c("lm", "rlm"), lty=c(2,1))

```

# $x$ measured without error

Let's see what happens if we assume that `pH.out` is measured without error, by
symply swapping the places of `month` and `pH.out` in the model formula.

```{r}
SI.out.xy.lm <- lm(month ~ pH.out, data=SI.data)
summary(SI.out.xy.lm)
```
### Question

  1. How different do you expect the lines to be?

### Plotting the prediction

```{r}
new.data <- data.frame(pH.out = seq(from = 7, to = 10, length.out = 120))
predict.xy.data <- cbind(new.data, 
                         predict(SI.out.xy.lm, new.data, interval = "confidence")) 
```

We first plot according to the formulation of the model we have fitted.

```{r}
ggplot(data = predict.xy.data) +
  geom_point(data = SI.data, 
             mapping = aes(y = month, x = pH.out)) +
  geom_ribbon(alpha = 0.25, color = NA, 
              mapping = aes(y = fit, x = pH.out, ymax = upr, ymin = lwr)) +
  geom_line(color = "red", mapping = aes(y = fit, x = pH.out))
```

The line in the plot above looks as expected. Even though we have fitted a 
linear regression of `month` on `pH.out` we will
still flip the _x_- and _y_-axes so that it is easier to compare to 
earlier figures.

```{r}
ggplot(data = predict.xy.data) +
  geom_point(data = SI.data, 
             mapping = aes(y = month, x = pH.out)) +
  geom_ribbon(alpha = 0.25, color = NA, 
              mapping = aes(y = fit, x = pH.out, ymax = upr, ymin = lwr)) +
  geom_line(color = "red", mapping = aes(y = fit, x = pH.out)) +
  coord_flip()
```

## Major axis regression

The example above only demonstrates how different the fitted lines can be, but
in practice, given the extreme outliers, it does not make sense to assume that
they are not exceptional. This does not necesarily mean that we can ignore or
remove these observations, as they may contain important information, but
possibly they will need to be studied as separate cases.

We will use package 'lmodel2' and a different example data set that is a real
use case. If you have data from _surveys_ where you want to study relationships
between measured variables that are subject to random variation then OLS linear
regression is likely to bias the slope estimates. For details read the _vignette_
that accompanies 'lmodel2'.

```{r}
library(lmodel2)
```

An example taken from the documentation.

```{r}
## Example 1 (surgical unit data)
data(mod2ex1)
Ex1.res <- lmodel2(Predicted_by_model ~ Survival, data=mod2ex1, nperm=99)
Ex1.res
plot(Ex1.res) 
```

```{r}
## Example 2 (eagle rays and Macomona)
data(mod2ex2)
Ex2.res <- lmodel2(Prey ~ Predators, data=mod2ex2, "relative", "relative", 99)
Ex2.res
op <- par(mfrow = c(1,2))
plot(Ex2.res, "OLS")
plot(Ex2.res, "RMA")
par(op)
```

