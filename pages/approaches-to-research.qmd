---
title: "_P_-values, _R_<sup>2</sup>, AIC, BIC"
subtitle: "How do they fit into the research process?"
author: "Pedro J. Aphalo"
date: 2023-08-19
date-modified: 2023-08-22
categories: [scientific method]
keywords: [research process, design of experiments, data analysis]
format:
  html:
    code-fold: false
    code-tools: true
    mermaid:
      theme: neutral
bibliography: design-exp.bib
abstract: 
  Fortuitously, in the last few days I have several times have had to ponder about the roles of hypothesis-based and descriptive approaches to research. Being an agronomist and ecologist rather than a philosopher, the question that worries me is how data analysis differs between these two approaches and how we should interpret the results of the analysis in each case. This page collects some of my thoughts and opinions on the subject _at the time of writing_.
draft: false
---

## Scientific research

The scope of this text is scientific research, which by definition seeks understanding, which is equivalent to describing mechanisms, or how the world works. The use of the [Scientific Method](https://plato.stanford.edu/entries/scientific-method/) separates science from pseudo-science. However, there are different views among philosophers of science about how narrow and strict the definition of _scientiifc method_ should be. 

Empirical approaches to prediction, are by definition judged by their predictive capacity, and based solely on correlations. They do not seek mechanistic understanding and are not discussed here.

I will not discuss the differences in reliabilty between mechanistic and purely empiric approches to prediction, their robustness or their usefulness. 
The discussion here centres on the role played observation in the acquisition of mechanistic understanding.

The world is too complex to be grasped as is through our limited mental capacity, but more fundamentally because the entity that attempts to achieve understanding is a small part of this whole. Thus this complexity can never by represented in all its detailed properties.

Scientific research works by simplification or abstraction, attempts to separate important from unimportant features of the world. What is important vs. unimportant depends on the context. Nothing is in every possible respect, at every possible temporal and spatial scale irrelevant. Importance of an event, observation or function can be decided only after we set a frame of reference. The frame of reference is determined by temporal and spatial scales and an aim: which phenomenon we want to explain or understand. To ensure relevance and practical usefulness, the scale at which the research is carried out and the scale of the phenomenon we want to explain need to at least partly overlap. If there is no overlap conclusions about the connection between the observations and the phenomenon remain subjective rather than supported by scientific evidence, i.e., any statement of usefulness or explanatory value would be based of faith instead of evidence and thus unscientific.

Another consequence of knowledge based on abstraction or simplification is that it is tentative and subject to revision. Not only because observation of previously unobserved events adds new information, but also because we may need/want to revise the frame of reference for the problem under study. Controversies in science in many cases are not the result of disagreement on the validity of observations but instead caused by disagreement about what frame of reference to use or by the reliance on poorly defined frames of reference. For example, there are many different definitions of _stress_ in use, each leading to a different frame of reference for the study of responses to _stress_ and the mechanisms involved in these responses.

## Two approaches

The currently most accepted views on the [Scientific Method](https://plato.stanford.edu/entries/scientific-method/) base it on a hypotheco-deductive (H-D) approach. Observational-Inductive (O-I) approaches are usually considered not to provide strong enough evidence. However, this does not mean that they do not play a key role in scientific research. Many statisticians, starting with John Tukey (@Friendly2022), have argued that the O-I approach plays a more important role in data analysis and scientific advancement than H-D approaches. 

A strict Popperian H-D approach would imply that all valid scientific knowledge derives from hypothesis testing (<font color=green>green</font> in @fig-research-process-flowchart). But where would the hypotheses originate? There would be no progress of innovation without the O-I approach and the imagination of hypotheses. It is also significant, that testing yields a probabilistic answer unless we are able to test all possible events of interest. However, we can never test all relevant events and at the same time extend the range of situations to which knowledge applies, such as using knowledge from current research to explain past and/or future events.

Consequently, we also use an approach based on looking/searching for consistent patterns in the observed world (<font color=blue>blue</font> in @fig-research-process-flowchart). The role of hypotheses in this case is much weaker, just a viewpoint that guides where we put the focus of the exploration of the world.

John Tukey rightly emphasized in his writings the difficulties involved real-world tests of hypothesis (@Friendly2022) compared with an idealised view where the outcome from a test of hypothesis is a binary, yes or no, answer. In practice, the outcome is always probabilistic and dependent on assumptions. Moreover, he cogently argued that the idea of even considering that any intervention/treatment can have absolutely not effect, i.e., to the highest degree of precision, just nonsensical. This is the background for his view, currently largely shared by statisticians, that the O-I approach plays a central role and that the difficulties in the practical application of the H-D approach must be always kept in mind. A crucial one is that the concept of _accepting the null hypothesis_ is fundamentally flawed.

::: callout-caution
From an operational perspective, which approach we use determines how we can analyse the data. Most importantly the approach we use also informs what type of conclusions we can reach and what criteria we should use to reach these conclusions.
:::

```{mermaid}
%%| label: fig-research-process-flowchart
%%| fig-cap: A diagram showing the steps of scientific studies. The thick arrows describe the sequence of events/actions, connecting the design of an experiment to the communication of the results. Two paths, <font color=green>**1.** for hypothesis based research</font> and <font color=blue>**2.** for descriptive studies</font>, are highlighted (see main text). The dotted arrows with round heads indicate constraints imposed by design-related decisions. The double headed dotted arrow describes that the realization of a study can be influenced by data observed during its course, especially when data are collected repeatedly. Thin arrows indicate how one study can affect subsequent studies. _QC_= Quality Control or sanity checks of data. Even when no hypothesis testing is done, a hypothesis of what variables are of interest is involved in deciding what data are going to be used or collected. _Only if no formal hypothesis testing is involved, we can revise this weaker hypothesis during data analysis._ This abstraction can be applied to empirical research, but with small changes (not shown) also to simulation studies.
%%{init: {"htmlLabels": true} }%%

flowchart TD
  
  Z([background<br>information]) ==> Y(Hypothesis)
  Y ==> A(Design) ==> Aa(Planning) ==> B(Realization) ==> H(Data collection) ==<font color=blue><strong>2.</strong>==> C
  C[<font color=blue><i>full</i> <strong>EDA</strong>] ==> D(<font color=blue>Model\nSelection) =="<font color=blue><i>R</i><sup>2</sup>, <i>f(x<sub>i</sub>)</i>, AIC, BIC"==> E(Interpretation) ==> F([communication])
  H ==<font color=green><strong>1.</strong>==> I[<font color=green><i>QC</i> <strong>EDA</strong>]
  H ==deposit<br>data+metadata=====> X([data<br>repository])
  I ==> G(<font color=green><strong>CDA</strong>\nTests of\nHypothesis) ==<font color=green><i>P</i>-value==> E
  E --follow up<br>study--> Y
  C <--<font color=blue>new/modified<br>hypothesis--> Y
  C --improved design--> A 
  I --improved design--> A
  B <-.-> H
  A -.-o D
  A -.-o E
  A -.-o G
  F --scientific<br>literature--> Z
  X --"open data"--> Z
  Z ==> E
  linkStyle 5,6,7,14,15 stroke: blue
  linkStyle 9,11,12,16 stroke: green
```

If we follow the O-I approach, how we treat data changes compared to the H-D approach: we explore the data with an open mind, rather than only as a source of information to make a decision about aout an _a priori_ hypothesis.

::: callout-tip
# In the words of F. Mosteller and J. W. Tukey

â€¦ data analysis, like calculations, can profit from repeated starts and fresh approaches; there is not just one analysis for a substantial problem. [@Mosteller1977]
:::

This quotation also highlights that frequently we choose among possible data analysis approaches in a rather subjective manner, mostly based on previous experience and expertise. These approaches may involve different assumptions, whose fulfilment in many cases cannot be tested from the data being analysed.

## Which approach is better?

I will start with what seems self evident to me. Neither H-D-based experimental research nor O-I research is better, both need to be combined for original scientific knowledge or technical know-how to be generated.

  1. Even when we think we use only one of these approaches, even if informally, we are using both. Why? Because new hypotheses do not come out of thin air! Because, when describing something new we always need something already known as a reference! Of course one approach may be emphasized at the expense of the other, or only one of them may be formally used and explicitly described and the other may participate implicitly and remain undescribed.

  2. Scientific research usually works by alternatively emphasizing each of the two approaches, although it is also possible to use them in parallel. This seems to be true for every branch of science, from Physics to Humanities.

  3. Simplifying the process to its bare bones, observation suggests hypotheses (= triggers in our mind possible explanations for observed phenomena) and testing selects from these hypotheses those which appear most likely to be true within a specific context or frame of reference. Thus, we never test all possible explanations, only those we have been able to imagine from our exposure to previous observations or other experience.
  
::: callout-note

# Charles Darwin and evolution

The idea of evolution by natural selection preceded Darwin's publication of the Origin of Species. The developent of the hypothesis of evolution by Darwin is usually timed to Darwin's travel around the world on the Beagle. Frequently it is attributed to his observations as naturalist, emphasizing the species he encountered in the Galapagos. There is an alternative explanation: on board the ship there was a library with at least a book that presented some of the same ideas. He did indeed write the first notes about evolution on board the Beagle, but the role of his previous academic contacts while a student and before the trip on the Beagle are now thought to have made this synthesis possible. Related ideas had been considered by philosophers and naturalists over the previous centuries, and Darwin was aware of at least some of these. Even Erasmus Darwin, Charles Darwin's grandfather had written about them.

Why does then Darwin get all the credit? He framed these ideas into a coherent and credible phenomenon. This was possible in part because he limited himself to a more restricted problem than his predecessors: he did not deal with the controversial question of the origin of life: for his theory, that populations or living organisms exists and grow was an axiom. In addition he spent most of his life looking for evidence to support evolution by natural selection in different groups of organisms. 

Looking back into his time, it was quite a feat to make a convincing case for evolution in the absence of an understanding of genetics or molecular biology. There was no known mechanism of how traits could be inherited from parents to siblings. At a higher level of organization, of course, there was evidence for trait inheritance documented in relation to plant and animal breeding, a literature Darwin was also familiar with.

See [Evolutionary Thought Before Darwin](https://plato.stanford.edu/entries/evolution-before-darwin/), [Darwin: From Origin of Species to Descent of Man](https://plato.stanford.edu/entries/origin-descent/), and [Darwinism](https://plato.stanford.edu/entries/darwinism/) for the details.
:::

## Differences among disciplines and problems

The subjects of study of different disciplines differ in complexity and in the reasons behind this complexity. The effort needed to test hypotheses, thus also depends on the disciplines, and in some crucially important fields, like medicine and environmental science it is frequent that direct tests of hypotheses are impossible, either by physical, temporal, spatial or ethical constraints. Taking this into account, it should be not a surprise that the approaches predominantly used and emphasis on either the O-I or H-D approach depends on the discipline and subject under study.

Usually, the more constrained the frame of reference is, the easier it is to apply the H-D approach but also narrower the range of validity of our conclusions. When we study very large and complex systems, the H-D approach becomes difficult to apply, simply because it is difficult or impossible to manipulate the factors we want to study. Sometimes, we can use a weaker version of the H-D approach, that at its extreme is not much more than the O-I approach presented as if it were H-D.

Some of the most urgent problems faced by humankind, like global climate change, can be mainly studied using the O-I approach. We cannot apply the H-D approach in full, because as researchers we cannot change the variables we hypothesise to be the drivers of global change. The use of the H-D approach is limited to small parts of the system, or to mathematical models that have been developed using at least in part the O-I approach.

## Small-, mediun- and big-sized data

Frequently, the distinction between small, medium and big data is data relies on the number of numeric values a data set contains. This can be useful from a computational perspective, but not from a data analysis perspective. I use here a different criterion, the number of significance tests or contrasts relative to the number of independent replicates.

  1. Big data are normally analysed with methods that do not consider statistical significance. The reason is that in this case statistical significance does not help at the time of making a decision. With thousands or even millions of replicates, random variation in the estimates is always very well controlled (because $S^2_{\overline x} = S^2_x / n$), and very small effects are statistically significant. Bias is much more difficult to control and "measure", specially because in most cases the sampling behind big data is not a perfectly aleatory process.

  2. Medium sized data, has enough replicates to meaningfully test significance assuming that experiments or surveys are well randomised in all relevant aspects. In this case, _P_-values inform us about the probability of observing the observed outcome assuming that the null hypothis is true. The null hypothesis provides a reference condition to compute the _P_-value, and is most freqeuntly "no effect". With multiple comparisons, in most cases we aim at controlling the number of false positive outcomes per experiment. We achieve this by adjusting the _P_-values upwards based on assumptions specific to each method. 

  3. From the perspective of data analysis, RNAseq data is extremely small: we study the response of thousands of genes, based on a handful of replicates. The data can be analysed only by assuming that the variation in expression among genes within a single replicate informs about variation in gene expression of an individual gene among replicates. In the case of multiple comparisons, we attempt to control the probability of false positive outcomes only in relative terms to the number of "positive outcomes". In this case, we use the false discovery rate, to adjust _P_-values.

## Null hypothesis and its problems

John Tukey argues that _lack of effect_ is a practical impossibility. Thus, accepting this as the result of a test is nonsensical. So we may reject or not the null hypothesis, but non-rejection does not mean acceptance, it means not enough information is available to make a decision. 

::: callout-tip
# In the words of John W. Tukey

_Statisticians classically asked the wrong questionâ€”and were willing to answer with a lie, one that was often a downright lie. They asked "Are the effects of A and B different?" and they were willing to answer "no."
All we know about the world teaches us that the effects of A and B are always differentâ€”in some decimal placeâ€”for any A and B. Thus asking ``Are the effects different?'' is foolish.
What we should be answering first is ''Can we tell the direction in which the effects of A differ from the effects of B?'' In other words, can we be confident about the direction from A to B? Is it "up," "down" or "uncertain"?
The third answer to this first question is that we are "uncertain about the direction"â€”it is not, and never should be, that we "accept the null hypothesis."_
[@Tukey1991]
:::

This leads to two questions: 1) does the null hypothesis need to be _no effect_, and 2) how should we validly interpret the results from statistical tests?

When we set a null hypothesis, in principle we can set it to any value instead of zero. In other words test for significance against a size of response that is of interest. This is rarely done in practice, except for testing if a slope differs from one.

Even in Bioinformatics this is not the usual approach, we tend to test for significance compared to zero change in expression and simultaneously require a minimum size of the response, usually a fold-change in expression. This is different to testing that the fold-change is significantly larger than an hypothesized value, which in most cases is a more stringent test.

Practical considerations play a role in the choice of approaches. With small data and using FDR we will get false positives, and we hope that many false positives will be in the small effects that we discard preventively. With big data unless we require a minimum size for the responses of interest, we cannot distinguish what is important from what is not.

## What does a small _P_-value tell us? 

The usefulness as a criterion of the _P_-value depends on the size of the data.

  1. In the case of big data, _P_-values do not tell anything useful. We should ignore _P_-values and base our interpretation on how much of the variation is explained by different explanatory variables. For example using partial correlations, AIC or BIC, and the relative importance of explanatory variables measured as the fraction of the variation explained.

  2. In the case of medium-sized data and simple assumed responses, traditionally _P_-values for main effects and interactions together with the use of adjusted _P_-values for multiple comparisons has been the preferred approach. When dose responses or time courses have complex shapes, setting a mathematical formulation for the shape of the response curve describing an _a priori_ hypothesis can be extremely challenging and simultaneously uninformative for complex systems. In such cases model selection as described above in 1. is more useful.

  3. In the case of small data, individual outcomes based on FDR must be taken with a grain of salt. Say with an FDR of 5%, if we get 1000 positive outcomes, 50 out of them can be expected to be false positives. This means, that in the case of gene expression assessed with arrays or by RNAseq, looking at the enrichment of metabolic pathways or processes, provides more reliable information than the outcomes for individual genes.
  
::: callout-note
# _P_-values

In recent years the use of _P_-values in research has been under debate. At the very least we need to assess if they are informative or not, data set by data set, taking into consideration the aims of each study and available replication. I think that one can safely say that _P_-values are currently overused in scientific reports and too frequently misinterpreted.

The American Statistical Society released an official statement [@Wasserstein2016] against the predominance of significance tests and _p_-values as a core part of Statistics practice and teaching:  [The ASA statement](https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108#.Xb_1B9VS9aQ). 

See also the blog posts [After 150 Years, the ASA Says No to _P_-values](https://matloff.wordpress.com/2016/03/07/after-150-years-the-asa-says-no-to-p-values/) and  [Further comments on the ASA manifesto](https://matloff.wordpress.com/2016/03/09/further-comments-on-the-asa-manifesto/) by Norman Matloff.
:::

## Are negative test outcomes, or negative results of any use?

Negative results can be useful and should be published, but only if they are informative. A high _P_-value by itself is not informative. As discussed above, it does not tell us the cause behind the lack of statistical significance: low replication, large uncontroled variation or small size of effect or difference under study.

Statistical power analysis is the tool that helps us out of this difficulty. Statistical power measures the sensitivity of a past or of a planned experiment towards detecting treatment or group differences. A post-mortem power analysis can be used to estimate the probability of effects of an arbitrary size having been detected in an experiment. So, even if as discussed above, it makes no-sense to accept the idea of no-effect or accepting the null hypothesis, we can get an idea how small a response would have had a high probability of having been detected by our study. This can be extremely useful to know.

The other side of the coin is that if we can estimate at the planning stage the error variance, and we have a target minimum size of response we want to be able to detect, we can compute how many replicates we need to achieve the desired level of sensitivity.

One can understand why journal editors are reticent to publish reports of negative results from experiments. However, rarely editors or authors are aware that through application of post-mortem power analysis it is possible to assess if negative results were caused by poor experimental design or by the small size of responses. Power analysis is infrequently taught or even mentioned in introductory Statistics courses.

A different approach is to stop using _P_-values and use confidence intervals (CI) for the estimated effects or parameter estimates. These have the advantage they show at a glance the value of an estimate and how much we can trust that this value is representative of that in the population sampled. _Many researchers use in plots standard errors of the mean instead of CIs because shorter error bars make the plots look nicer, even if not as easy to interpret._

If CIs are used in a figure to assess significance through implicit multiple comparisons, they should be based on adjusted _P_-values. These "adjusted" CIs are frequently called _simultaneous CIs_.

## Data analysis as a soft skill

Statistics gives the formal support to data analysis, but data analysis and design of experiments are in many respects acquired skills. They both involve un-directed open-minded observation as well as imagination. They are creative activities and to an extent subjective in their execution but if done respecting common sense and the principles of statistics, they allow us to learn how the world works.

The link between reality and scientific knowledge has been debated by philosophers for a long time. Most researchers disagree with the idea that scientific knowledge is a construct disconnected from the real world. On the other hand, it is difficult to support the idea that scientific knowledge reflects only real world untainted by how and why we study it. My own view is that even if scientific knowledge describes the real world, it is also influenced by researchers' views and decisions. I tend to think that even though scientific knowledge describes in an abstract way real events, objects and relationships that exist independently of the observer, how we describe them or imagine them is not unique. As abstractions are simplifications, they only represent a portion of the total reality. Thus, different abstractions may coexist and be valid within their own frames of reference. So testing is possible, within a frame of reference or context that is part of an hypothesis.

To a smaller or larger extent, the world view of researchers affects the hypothesis they more readily imagine, but these biases tend sooner or later to be sorted out by deeper theoretical analysis and experimentation or observation. An obvious case has been the contrasting emphasis put on competition vs. facilitation among ecologists.

## Further reading

  - _The Sunset Salvo_ [@Tukey1986] is a sobering medicine for those with blind faith in Statistics and the objectivity of data analysis.
  
  - The article _Prediction, Estimation, and Attribution_ [@Efron2020] discusses in more depth, but still accessibly, the differences between traditional data analysis and "large-scale" prediction algorithms as used in "machine learning (ML)" and "artificial intelligence".

  - The books Planning of Experiments [@Cox1958] and Statistics and Scientific Method [@Diggle2011] can also be recommended as they focus mainly on the logic behind the different designs.

  - The book _Modern Statistics for Modern Biology_ [@Holmes2019], is true to its name, a modern account of Statistics that takes a broad view including extensive use of data visualizations, It is specially well suited to those interested in molecular biology as it includes the statistics behind bioinformatics. In other words, this books presents statistics in the context of biological data analysis.
  
  - The booklet _The Guide-Dog Approach_ [@Tuomivaara1994] proposes a middle ground in the philosophy of science controversy, as applied to Ecology. [Mario Bunge](https://en.wikipedia.org/wiki/Mario_Bunge), a philosopher of science who started his scientific career as a researcher in quantum physics, has written very extensively on philosophical questions related to science: what is knowable, how to understand cause and effect relationships, and how much of the knowledge we acquire is a reflection of ourselves, individually or collectively, versus a description of the real world as it is independently of us, the observers. In _Chasing Reality: Strife over Realism_, @Bunge2014 brings together some of the ideas from his long career. Among other things he highlights the role of "disciplined imagination" in scientific research, something he has written about earlier, even considering the role of the reading of fantastic literature as a way of developing imagination skills in future scientists and technicians.
  
