---
title: "Model fitting in R"
subtitle: "An introduction"
author: "Pedro J. Aphalo"
date: 2023-05-30
date-modified: 2023-06-29
categories: [R, model fitting]
keywords: [predicted values, residual values, parameter estimates, model formulas]
format:
  html:
    code-fold: true
    code-tools: true
---

## Introduction

Fitting a model to data consists in finding the parameter values that best
explain the data or observations. These parameter values are estimates of
those in a larger population of possible observations from which we have
drawn a sample or subset.

::: callout-tip
You may want to play with the my 
[linear regression Shiny App](https://aphalo.shinyapps.io/class01-practical/#21) 
to get a better sense of how the size of the response and error variation 
affect the linear model fit to succesive sample draws from a normally
distributed population of artificial observations.
:::

Model selection involves comparing models that differ in their structure, i.e., 
in the formula that relates parameters and data.

In general, there are many different aspects of a model fit that we may be
interested in. The most computationally intensive step is the fitting itself.
Thus R's approach is to save the results from fitting, or _fitted model_, and
separately query it for different derived numerical and graphical output as 
needed.

## Fitting a model

::: callout-tip
In this page code chunks are "folded" so as to decrease the clutter. Above the R
output, text or plots, you will find a small triangle followed by "Code".
Clicking on the triangle "unfolds" the code chunk making visible the R code used
to produce the output shown.
The code in the chunks can be copied by clicking on the top right corner, where
an icon appears when the mouse cursor hovers over the code listing.
:::

```{r, message = FALSE}
library(ggpmisc)
library(broom)
theme_set(theme_bw(16))
```

```{r}
set.seed(19065)
# set.seed(4321) # change or comment out to get different psedorandom values
# generate artificial data
x <- 1:24
y <- (x + x^2 + x^3) + rnorm(length(x), mean = 0, sd = mean(x^3) / 2)
y <- y / max(y)
my.data <- data.frame(x, 
                      y, 
                      group = c("A", "B"), 
                      y2 = y * c(1, 2) + c(0, 0.2),
                      block = c("a", "a", "b", "b"),
                      wt = sqrt(x))
```

In R we usually save the fitted model object into a variable.

```{r}
fm1 <- lm(formula = y ~ poly(x, 3), data = my.data)
```

Then we query this object saved in the variable, here `fm1`, with different methods, 
for example `summary()`.

```{r}
summary(fm1)
```

## Model fitting flowcharts

```{mermaid}
%%| label: fig-barebones
%%| fig-cap: A minimalist diagram of model fitting in R.
%%{init: {"htmlLabels": true} }%%

flowchart LR
  A(<i>model formula</i>) --> B[model fit\nfunction] --> C(model fit\nobject) --> D1['diagnostics' plots]
  AA(<i>observations</i>) --> B
  C --> D2[query methods]
```


```{mermaid}
%%| label: fig-lm
%%| fig-cap: A diagram of linear-model (LM) fitting in R.
%%{init: {"htmlLabels": true} }%%

flowchart LR
  A1(<i>model formula</i>) --> B["<code>lm()</code>"] --> C(<code>lm</code> object) --> C1["<code>plot()</code>"]
  A2(<i>observations</i>) --> B
  A3(<i>weights</i>) -.-> B
  C --> C2["<code>summary()</code>"]
C --> C3["<code>anova()</code>"]
C --> C4["<code>residuals()</code>"]
C --> C5["<code>fitted()</code>"]
C --> C6["<code>AIC()</code>"]
C --> C7["<code>BIC()</code>"]
C --> C8["<code>coefficients()</code>"]
C --> C11["<code>formula()</code>"]
C --> C12["<code>weights()</code>"]
C --> C9["<code>confint()</code>"]
C --> C10["<code>predict()</code>"]
BB("<i>new data</i>") --> C10
```

```{mermaid}
%%| label: fig-glm
%%| fig-cap: A diagram of generalized-linear-model (GLM) fitting in R. Query methods as in @fig-lm.
%%{init: {"htmlLabels": true} }%%

flowchart LR
  A1(<i>model formula</i>) --> B["<code>glm()</code>"] --> C(<code>glm</code> object) --> C1[query methods]
  A2(<i>observations</i>) --> B
  A3(<i>weights</i>) -.-> B
  A4(<i>family</i> and <i>link</i>) --> B
```

```{mermaid}
%%| label: fig-nls
%%| fig-cap: A diagram of nonlinear least squares (NLS) model fitting by numerical approximation in R. Query methods similar to those in @fig-lm.
%%{init: {"htmlLabels": true} }%%

flowchart LR
  A1(<i>model formula</i>) --> B["<code>gls()</code>"] --> C(<code>nls</code> object) --> C1[query methods]
  A2(<i>observations</i>) --> B
  A3(<i>weights</i>) -.-> B
  A5(<i>starting values</i>) --> B
```

## Model selection flowchart

Model selection can be done manually by comparing models fitted individually or automatically using a stepwise approach.

```{mermaid}
%%| label: fig-lm-step
%%| fig-cap: A diagram of linear-model (LM) fitting with stepwise model selection in R.
%%{init: {"htmlLabels": true} }%%

flowchart TB
  A1(<i>model formula</i>) --> B["<code>lm()</code>"] --> C(<code>lm</code> object)
  A2(<i>observations</i>) --> B
  A3(<i>weights</i>) -.-> B
  C --> CI[query methods]
  C --> C1["<code>step()</code>"]
  subgraph Z ["<strong>Model selection</strong>"]
  C1 --> C3(<code>lm</code>  object)
  z1(most complex\n<i>model formula</i>) -.-> C1
  z2(simplest nested\n<i>model formula</i>) -.-> C1
  C3 --> CF[query methods]
  end
  style Z fill:#fff
```

## What do the query methods return?

I expect you to be already familiar with the ANOVA table, coefficient estimates and plots of residuals. But how does all this fit together? I will use plots to explain this.

The observations.

```{r}
ggplot(my.data, aes(x = x, y = y)) +
         geom_point()
```

The fitted values.

```{r}
ggplot(my.data, aes(x = x, y = y)) +
         geom_point() +
  stat_fit_fitted(formula = y ~ poly(x, 3), colour = "blue")
```

The prediction line and its equation.

```{r}
ggplot(my.data, aes(x = x, y = y)) +
         geom_point() +
  stat_poly_line(formula = y ~ poly(x, 3), se = FALSE) +
  stat_poly_eq(formula = y ~ poly(x, 3),
               mapping = use_label("eq"))
```

The prediction line and $R^2$, $n$, $F\textrm{-value}$ and $P\textrm{-value}$.

```{r}
ggplot(my.data, aes(x = x, y = y)) +
         geom_point() +
  stat_poly_line(formula = y ~ poly(x, 3), se = FALSE) +
  stat_poly_eq(formula = y ~ poly(x, 3),
               mapping = use_label(c("R2", "n", "F", "P")))
```


The prediction line plus its 95% confidence band.

```{r}
ggplot(my.data, aes(x = x, y = y)) +
         geom_point() +
  stat_poly_line(formula = y ~ poly(x, 3), se = TRUE)
```

The prediction line plus its 95% confidence band with extrapolation.

```{r}
ggplot(my.data, aes(x = x, y = y)) +
         geom_point() +
  geom_vline(xintercept = range(my.data$x), size = 0.33) +
  stat_poly_line(formula = y ~ poly(x, 3), se = TRUE, fullrange = TRUE) +
  expand_limits(x = c(-10, 40)) # an arbitrary range
```

The residuals plotted as deviations from the prediction line.

```{r}
ggplot(my.data, aes(x = x, y = y)) +
         geom_point() +
  stat_poly_line(formula = y ~ poly(x, 3), se = FALSE) +
  stat_fit_deviations(formula = y ~ poly(x, 3), colour = "red", 
                      arrow = arrow(length = unit(0.33, "lines"), ends = "both"))
```


The residuals plotted as deviations from the fitted values.

```{r}
ggplot(my.data, aes(x = x, y = y)) +
         geom_point() +
  stat_fit_deviations(formula = y ~ poly(x, 3), 
                      colour = "red", 
                      arrow = arrow(length = unit(0.33, "lines"), ends = "both")) +
  stat_fit_fitted(formula = y ~ poly(x, 3), colour = "blue")
```

The residuals plotted on their own.

```{r}
ggplot(my.data, aes(x = x, y = y)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  stat_fit_residuals(formula = y ~ poly(x, 3),
                     colour = "red")
```


::: callout-warning
The assumption of most usual model fitting procedures is that residuals are normally distributed and independent of the magnitude of the response variable, not that the observations themselves are normally distributed. Only in the simplest cases, such as comparing a mean against a constant, the residuals have the same distribution as the data but centred on zero instead of on the mean. In most other cases, this is not the case, as part of the variation among individual observations is accounted by terms in the fitted model, such as random effects, correlations or even variance covariates. The justification is that we use the residuals to estimate the _error_ variation and the tests of significance are based on this error variance estimate.

It is also of little use to test for the statistical significance of the deviations from these assumptions, as we should not expect in the real world for the assumptions to be ever exactly fulfilled. The power of tests increases with replication, but the bias introduced into estimates does not depend on significance, but on the magnitude of the deviations. Furthermore, the higher the replication, the less the bias introduced in the estimates by deviations from the assumptions. In other words, the more replicates we have smaller deviations from assumptions become detectable as significant, while the importance of deviations decreases.

When there are very few replicates available, it is imposible to assess directly if observations come from a normally distributed population or not. In such cases, it can be wise to rely on previous information about the sampled population and sampling method used instead of on the current observations.
:::

::: callout-tip
To create the plots above I used packages 'ggplot2' and 'ggpmisc'. [Galleries
of plot examples using 'ggpmisc' and some other
packages](https://www.r4photobiology.info/galleries.html) are available at the
[R for Photobiology web site](https://www.r4photobiology.info). They contain R
code folded in the same way as in this page.
:::
