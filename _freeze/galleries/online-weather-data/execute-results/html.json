{
  "hash": "5989a4d3ac297c98744efa765cfd4118",
  "result": {
    "markdown": "---\ntitle: \"Open Access Weather and Climate Data\"\nsubtitle: \"Importing qne using them in R\"\nauthor: \"Pedro J. Aphalo\"\ndate: \"2023-07-31\"\ndate-modified: \"2023-08-01\"\ncategories: [Using R]\nkeywords: [R packages, weather data]\ncode-fold: true\nformat:\n  html: \n    code-link: true\n    code-tools: true\nbibliography: hints.bib\nabstract: |\n  Example R code to download, import and plot wheather and climate data for Europe and the whole Earth. The data bases used in the examples are E-Obs, TerraClimate and NASA Power.\n---\n\n\n::: callout-warning\nThis page is still a draft. Code examples do work but will be expanded and the\ntext will be revised.\n:::\n\n::: callout-tip\nIn this page code chunks are \"folded\" so as to decrease the clutter when\nsearching for examples. Above each plot you will find one or more \"folded\" code\nchuncks signalled by a small triangle followed by \"Code\". Clicking on the\ntriangle \"unfolds\" the code chunk making visible the R code used to produce the\nplot.\n\nThe code in the chunks can be copied by clicking on the top right\ncorner, where an icon appears when the mouse cursor hovers over the code\nlisting.\n\nThe `</> Code` drop down menu to the right of the page title makes it possible to unfold all code chunks and to view the Quarto source of the whole web page.\n\nNames of functions and other R objects are linked to the corresponding on-line help pages. The names of R extension packages are linked to their documentation web sites when available.\n:::\n\n::: callout-note\nThe content of this page was originally written for the _Hints and Tips_ section of the [UV4Plants Bulletin](https://bulletin.uv4plants.org/) but was not published as the Bulletin ceased to be published.\n:::\n\n## Introduction\n\nA problem faced when designing studies comparing plant responses over broad\nregions is how to obtain consistent and reliable weather and climatological\ndata. Inspired by Danielle Griffoni's presentation at the most recent\nUV4Plants network meeting and my own need for such data, I\nhave chosen as the subject of the present column _Weather and\nclimate: Open data sources_. By necessity the contents of this article deals\nwith just a few examples out of the many data sources available, and is far\nfrom being comprehensive. The included code examples are in R, simply because\nR is the language I use most frequently. Frequent users Python and other\ncomputer languages would surely be able to produce similar code examples in\ntheir favourite language.\n\nWe are in the era of _Big Data_ and _Open Data_ but how can we\nfind, download and use these data? As usual, I will use R for the examples.\nMost spatially and temporarily indexed data are nowadays stored in a file\nformat called NetCDF4, or the fourth update of the NetCDF format's\ndefinition. The key advantage of these files is that they can be read\nselectively. In other words, data can be read and used from files that are\nlarger than the available computer memory. This is crucial because for daily\ndata on a dense grid, such as those at 1/24$^\\circ$ of latitude and\nlongitude, with global or continental coverage and many decades of data, a\nsingle variable can require tens of giga bytes of storage. How much of the\ndata included in these sets one will actually use depends on the number of\nsites and length of time of interest, and in many cases it is only a tiny\nportion of the whole data set.\n\nRecently I was working with daily data for nearly $1\\,000$ sites over more than\n60 years, producing multiple rolling and accumulated summaries in R and I ended\ncreating a data frame ($\\approx$ a worksheet) containing nearly\n$1\\,000\\,000\\,000$ values! The bright side is that I could do this processing\nboth on my four-years-old slim laptop (ThinkPad T395s) and on a 12-years-old\ndesktop PC (a Lenovo ThinkStation E30 from 2011). Of course, reading and\nprocessing the data are time-consuming. In this case my script took over 1/2 a\nday to run for each meteorological variable read and summarised.\n\nGetting access to such a trove of curated and validated meteorological data\nopens the door to many data analyses, while on the other hand, as data on UV\nirradiance not weighted with the erythremal action spectrum is rather scarce,\nthings are not as easy for UV research as one would like. Global radiation\nand ozone column are more easily obtainable, opening the door to radiation\ntransfer modelling.\n\n## The NetCDF format\n\n_Network Common Data Form_ (NetCDF) files are described as\nself-descriptive, machine and computer-language independent, scalable,\nappendable, and shareable. What does this all mean in practice?\n\nNetCDF files contain meta-data indicating the units of expression of the\nvalues stored and a short description for each variable. Some variables are\ntagged as describing the grid on which data are stored, for geographical data\nthe grid is given by latitude and longitude. In the case of meteorological\ndata, time adds a third dimension to the grid. For transcriptome data from a\nmicroarray the grid will describe the coordinates the ``cells'' in the array.\n\nA NetCDF file saved under one operating system (e.g., Linux, Windows, etc.)\nor computer architecture (e.g., Intel 8086, ARM, etc.) can be read and\nwritten to under any other. This works not only locally, but also remotely\nthrough the internet. Mulitple ``clients'' can read the same file\nsimultaneously, locally or remotely, and selectively.\n\nNetCDF files can be huge (several gigabytes in size), but as they can be\nselectively read from and written to based on positions along grid\ndimensions, even remotely, they avoid unnecessary data transmission if read\nremotely, or the need for enough RAM capacity to read the whole file at once.\nAs the files can be reopened to incrementally add data, once again, this can\nbe done efficiently and frequently. Remote access allowing selective download\nis implemented using the [THREDDS](https://www.unidata.ucar.edu/software/tds/)\n, [OPeNDAP](https://www.opendap.org/)\nand similar DAP servers (DAP is an acronym\nfor data access protocol). These servers can be accessed both interactively\nthough a web browser or through software such as from R or Python scripts.\nWhen the file format is NetCDF some R packages make remote access consistent\nwith local file access, which is an advantage.\n\nThe use of the NetCDF file format is widespread for meteorological and\ngeophysical data, but also used in other fields. NetCDF files are well\nsupported by software as the core software libraries needed are available for\nfree and are open-source. The format was developed by \n[UniData](https://www.unidata.ucar.edu/software/netcdf/) primarily for storing\ngeophysical data.\n\n## Gridded data and interpolation\n\nOriginal data collected at ground level by weather stations are not available\non a regular grid, but instead stations are irregularly distributed in space,\nand measurements describe conditions at these specific locations. Data\nderived from satellite instrumentation are distributed following the overpass\ntracks and represent spatial averages as dictated by the resolution of the\ncameras or other instruments. The merging of data from multiple sources,\nusing models, receives the name of ``data assimilation''.\n\nMany of the currently available climatological data sets integrate data from\nmultiple sources, and re-express them on a regular grid to facilitate their\nuse. Spatial interpolation is used to achieve this, and obviously the\nuncertainty of the interpolated values as re-expressed on the grid depends on\nthe density of observations available as well as their uncertainty. The\ninterpolation process is also affected by topography, which even when\nexplicitly taken into account, adds to the uncertainty of the data. In these\ndata sets, the uncertainty varies among variables, as for example, more\nmeteorological stations have instrumentation for air temperature than for\nglobal radiation. Depending on the data set, long term trends may be\nmeaningful or not, as bias can be introduced by changes in the location of\nweather stations or more frequently by changes to their surroundings, say\nexpansion of the built-up area of a city. Some data bases include data\nderived from the primary observations, such soil moisture or climatic water\ndeficit. Obviously, when variables like potential evapotranspiration (PET) is\nderived from primary observations, uncertainties accumulate. As with any\nderived or interpolated data, it is important to understand their origin,\nuncertainties involved and the use they have been designed for. Data sets are\nprepared with an objective in mind and validated for a certain use. For\nexample, in some cases interpolation has prioritized the minimization or\ntemporal biases and in other cases it has not.\n\nOne final reminder: spatial grid density after interpolation does no longer\nreflect the spatial density of measurements. So if data are interpolated to a\ngrid with 1/24 degree resolution using data from stations that are 100's of\nkilometers away, what we gain by gridding is not new information but only an\ninformed guess of what may have been the conditions at a location for\nwhich there is no data available. Sometimes, other information is used\nto improve the predictions, such as the elevation above sea level at each\ngrid point. In any case, topography decreases the reliability of\nthe gridded estimates in mountainous regions.\n\n## Data use restrictions and acknowledgements\n\nThe data in each data base may have multiple origins and been subjected to\nvarious processing steps. There is much personal effort and money invested in\nthem, and proper recognition is due to authors. In practice, continued\navailability of these data resources depends on demonstration of their\nusefulness and contribution to scientific progress and practical applications\nbeing demonstrated. This makes crucial that every user without exception\ncorrectly acknowledges the use of open data sources.\n\nThe use of the data in these databases is subject to restrictions, for\nexample, E-OBS data cannot be used for commercial purposes, at least without\nobtaining explicit authorization. Each database has instructions about how\nthe source should be acknowledged. Do check these before use and make sure to\ncomply with acknowledgements as requested by the authors. These restrictions and\nspecially how to cite the databases can change in time. For example when the\ndata set is updated or when a new article is published describing the data or\nthe method used. This means that in most cases we should use the most recent\nversion of the data, and always check the current use restrictions and\ninclude the citation(s) and acknowledgement(s) as required or suggested at\nthe time we are preparing the publication-ready version of our manuscripts.\n\n\n## The data bases\n\nI will show how to retrieve data from several databases containing data of\ninterest for research with plants and vegetation. Most databases provide\nmultiple ways to retrieve data, in most cases using the NetCDF file format\ndescribed above. Interactive access with a browser is demonstrated for E-OBS\nin a video (Video S1), while here I demonstrate code-based data access,\ndownload as well as some post-precessing with examples using R.\n\n### TerraClimate\n\n[TerraClimate](https://www.climatologylab.org/terraclimate.html) provides \nmonthly data with worldwide coverage at a very high\nresolution (1/24 degree), and in addition to monthly summaries for weather\nmeasurements it contains several derived variables very useful for ecological\nstudies and agriculture (@tbl-terraclimate-vars). The data have\nbeen validated against different measurements, but it must be remembered that\nthe availability of primary data is very uneven across the Earth surface, and\nuncertainties vary accordingly. The data are derived from the assimilation of\nmultiple data sources, including both surface and remote measurements. It\nis important to be aware that although the data are provided on a very\ndense spatial grid, the original data are much more sparse, so the true\nspatial resolution is not as fine as one would think at first sight. On\nthe other hand the interpolated values can in many cases be the best\napproximation available to the unknown conditions at a given point in\nspace.\n\n| Variable   | Description                                                 |\n|:-----------|:------------------------------------------------------------|\n| aet        | Actual Evapotranspiration, monthly total |\n| def        | Climate Water Deficit, monthly total |\n| pet        | Potential evapotranspiration, monthly total |\n| ppt        | Precipitation, monthly total |\n| q          | Runoff, monthly total |\n| soil       | Soil Moisture, total column at end of month |\n| srad       | Downward surface shortwave radiation |\n| swe        | Snow water equivalent at end of month |\n| tmax       | Max Temperature, average for month |\n| tmin       | Min Temperature, average for month |\n| vpd        | Vapor Pressure Deficit, average for month |\n| vap        | Vapor pressure, average for month |\n| ws         | Wind speed, average for month |\n| PDSI       | Palmer Drought Severity Index, at end of month |\n\n: Variables in the TerraClimate data set. Variables names used in\n  the data set and brief text descriptions. The metadata included in the\n  netCDF files provides up-to-date information on the units and basis\n  of expressing and there encoding in the files. {#tbl-terraclimate-vars}\n\nTerraClimate also provides future climate projections developed for two\ndifferent climate futures: when global mean temperatures are (1) 2\\,$^{\\circ}$C and (2)\n4\\,$^{\\circ}$C warmer than pre-industrial, and (2) when global mean temperatures are 4\\,$^{\\circ}$C\nabove preindustrial. The future climate data are based on actual data from years\n1985--2015, which for the manipulated data are called pseudo-years. The\nvalues of variables for future climate are based on multiple simulation\nmodels.\n\nThis data base supports the OPeNDAP protocol which allows selective download\nfrom a remotely served NetCDF file, in addition to allowing download of the\nwhole data set and ready-made subsets. The data set has 37.3 million grid\npoints for 14 variables for each of 12 months for 61 years, i.e., $\\approx\n382\\times10^9$ data values plus the values describing each of the coordinates\nfor each grid point. This is Big Data.\n\nOnce one understands how to remotely read a NetCDF file, and how to select\nthe observations from the 3D grid (latitude, longitude and time), the R code\nneeded is rather simple. In this case we need to download variables one by\none. The information we need to know is the URL to use. In this database the\nURLs differ only by the name of the variable.\n\nPackage 'tidync' can be used to retrieve the data in one step directly getting a \n`tibble`. There is \na difficulty when retrieving data: we need to know the coordinates for the\ngrid points that are nearest to our target locations. In this example we will\nretrieve data for a single grid point and time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(tidync)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvar=\"tmax\"\n\ndata_url <- \n  paste0(paste0(\"http://thredds.northwestknowledge.net:8080/thredds/dodsC/agg_terraclimate_\",\n                var),\n         \"_1958_CurrentYear_GLOBE.nc\")\n\ntnc <- tidync(data_url)\nprint(tnc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nData Source (1): agg_terraclimate_tmax_1958_CurrentYear_GLOBE.nc ...\n\nGrids (5) <dimension family> : <associated variables> \n\n[1]   D2,D1,D3 : tmax    **ACTIVE GRID** ( 29113344000  values per variable)\n[2]   D0       : crs\n[3]   D1       : lat\n[4]   D2       : lon\n[5]   D3       : time\n\nDimensions 4 (3 active): \n  \n  dim   name  length     min     max start count    dmin    dmax unlim coord_dim \n  <chr> <chr>  <dbl>   <dbl>   <dbl> <int> <int>   <dbl>   <dbl> <lgl> <lgl>     \n1 D1    lat     4320   -90.0    90.0     1  4320   -90.0    90.0 FALSE TRUE      \n2 D2    lon     8640  -180.    180.      1  8640  -180.    180.  FALSE TRUE      \n3 D3    time     780 21184   44894       1   780 21184   44894   FALSE TRUE      \n  \nInactive dimensions:\n  \n  dim   name  length   min   max unlim coord_dim \n  <chr> <chr>  <dbl> <dbl> <dbl> <lgl> <lgl>     \n1 D0    crs        1     3     3 FALSE TRUE      \n```\n\n\n:::\n\n```{.r .cell-code}\n# a single grid point\nmy.lon <- 45.05\nmy.lat <- 45.01\n# we need to take into account how values in variable time are stored\nmy.month <- as.integer(ymd(\"2015-06-01\") - ymd(\"1900-01-01\"))\n\n# we use tests as arguments for lon and lat to find the nearest grid point\nhyper_tibble(tnc,\n             lon = index == which.min(abs(lon - my.lon)),\n             lat = index == which.min(abs(lat - my.lat)),\n             time = time == my.month,\n             na.rm = FALSE) %>%\n  # convert time in number of days to a date\n  mutate(date = ymd(\"1900-01-01\") + days(time),\n         year = year(date),\n         month = month(date),\n         tmax = ifelse(abs(tmax) == 32768, NA, tmax)) %>%\n  select(-time) -> grid_point.tb\n\n# print the result\ngrid_point.tb\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 6\n   tmax   lon   lat date        year month\n  <dbl> <dbl> <dbl> <date>     <dbl> <dbl>\n1  31.8  45.1  45.0 2015-06-01  2015     6\n```\n\n\n:::\n:::\n\n\nWe continue from the previous example retrieving data from all grid points\nwithin an area and for a time period. To save typing we define a function,\nwhich, thanks to its name, makes also clear the intention of the code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nis_within <- function(x, range) {\n  # protect from ranges given backwards\n  range <- range(range)\n  x > range[1] & x < range[2]\n}\n\n# we save the limits to variables\nmy.lon <- c(50, 50.5)\nmy.lat <- c(30.5, 31)\nmy.period <- as.integer(ymd(c(\"1960-03-01\", \"1960-09-01\")) - ymd(\"1900-01-01\"))\n\nhyper_tibble(tnc,\n             lon = is_within(lon, my.lon),\n             lat = is_within(lat, my.lat),\n             time = is_within(time, my.period),\n             na.rm = FALSE) %>%\n  # convert time in number of days to a date\n  mutate(date = ymd(\"1900-01-01\") + days(time),\n         year = year(date),\n         month = month(date),\n         tmax = ifelse(abs(tmax) == 32768, NA, tmax)) %>%\n  select(-time) -> grid_area_in_time.tb\n\ngrid_area_in_time.tb\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 720 × 6\n    tmax   lon   lat date        year month\n   <dbl> <dbl> <dbl> <date>     <dbl> <dbl>\n 1  27.8  50.0  31.0 1960-04-01  1960     4\n 2  27.5  50.1  31.0 1960-04-01  1960     4\n 3  26.6  50.1  31.0 1960-04-01  1960     4\n 4  23.2  50.1  31.0 1960-04-01  1960     4\n 5  23.8  50.2  31.0 1960-04-01  1960     4\n 6  26.0  50.2  31.0 1960-04-01  1960     4\n 7  26.8  50.3  31.0 1960-04-01  1960     4\n 8  26.7  50.3  31.0 1960-04-01  1960     4\n 9  26.6  50.4  31.0 1960-04-01  1960     4\n10  26.6  50.4  31.0 1960-04-01  1960     4\n# ℹ 710 more rows\n```\n\n\n:::\n:::\n\n\nIn principle we could download the whole files, and access them\nlocally, but the catch is in the size of the files: up to a few GB per\nvariable. For this reason, unless we plan to use a large chunk of the data,\nor repeatedly extract different subsets\nit is more efficient to use selective downloads.\n\n### E-OBS\n\nThe [E-OBS](https://surfobs.climate.copernicus.eu) [@Cornes2018] covers Europe plus\nnearby regions including land bordering the Mediterranean sea to the South\nand East. Data is daily and based on measurements at thousands of weather\nstations and extends at the time of writing from 1920-01-01 to 2023-07-30.\nThese data have estimates of uncertainty for each variable, grid and time\npoint. The maximum spatial resolution available is 1/10 degree for both\nlatitude and longitude. Download of data is free but requires registration.\nData is restricted to non-commercial use. In this case, after logging-in into\nthe web site one can download the NetCDF files to a local disk (Video:\nXXXXX). Once downloaded, as a second step, one can access the file as shown\nabove for TerraClimate data. The most reliable data are from 1950 to the\npresent, and at 1/10 degree resolution. Files contain data for a single\nvariable, and are large (6 to 15 GB each). Estimates of uncertainty are also\nin individual files, one per variable.\n\nWe can adjust the code shown above to work with these files. First step is to\nuse a file path instead of an URL. When exploring the properties of the file\nwe can notice some differences, even in the names of the grid variables and how\ntime is encoded (the starting date is different).\n\nOPeNDAP access is available at (http://opendap.knmi.nl/knmi/thredds/dodsC/)\nthrough a THREDDS server maintained by the Royal Netherlands Meteorological\nInstitute, but the version currently available is two years behind the\nCopernicus server of EU. With `catalog.html` listing the data sets and\nfor example the link ending in\n`e-obs_0.25regular/tg_0.25deg_reg_v17.0.nc` giving access to the variable\n`tg` with daily mean air temperature.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# We will use local file downloaded from web page\n# This file is nearly 800 MB in size but online query is cumbersome for this\n# server.\n\neobs_tnc <- tidync(\"data/tg_ens_mean_0.25deg_reg_v27.0e.nc\")\n\nactivate(eobs_tnc, \"tg\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nData Source (1): tg_ens_mean_0.25deg_reg_v27.0e.nc ...\n\nGrids (4) <dimension family> : <associated variables> \n\n[1]   D1,D2,D0 : tg    **ACTIVE GRID** ( 2486698032  values per variable)\n[2]   D0       : time\n[3]   D1       : longitude\n[4]   D2       : latitude\n\nDimensions 3 (all active): \n  \n  dim   name      length   min     max start count  dmin    dmax unlim coord_dim \n  <chr> <chr>      <dbl> <dbl>   <dbl> <int> <int> <dbl>   <dbl> <lgl> <lgl>     \n1 D0    time       26663   0   26662       1 26663   0   26662   TRUE  TRUE      \n2 D1    longitude    464 -40.4    75.4     1   464 -40.4    75.4 FALSE TRUE      \n3 D2    latitude     201  25.4    75.4     1   201  25.4    75.4 FALSE TRUE      \n```\n\n\n:::\n\n```{.r .cell-code}\nprint(eobs_tnc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nData Source (1): tg_ens_mean_0.25deg_reg_v27.0e.nc ...\n\nGrids (4) <dimension family> : <associated variables> \n\n[1]   D1,D2,D0 : tg    **ACTIVE GRID** ( 2486698032  values per variable)\n[2]   D0       : time\n[3]   D1       : longitude\n[4]   D2       : latitude\n\nDimensions 3 (all active): \n  \n  dim   name      length   min     max start count  dmin    dmax unlim coord_dim \n  <chr> <chr>      <dbl> <dbl>   <dbl> <int> <int> <dbl>   <dbl> <lgl> <lgl>     \n1 D0    time       26663   0   26662       1 26663   0   26662   TRUE  TRUE      \n2 D1    longitude    464 -40.4    75.4     1   464 -40.4    75.4 FALSE TRUE      \n3 D2    latitude     201  25.4    75.4     1   201  25.4    75.4 FALSE TRUE      \n```\n\n\n:::\n\n```{.r .cell-code}\n# we save the limits to variables\nmy.lon <- c(-30, -10)\nmy.lat <- c(55, 65)\nmy.period <- as.integer(ymd(c(\"2020-03-01\", \"2021-09-01\")) - ymd(\"1950-01-01\"))\n\nhyper_tibble(eobs_tnc,\n             longitude = is_within(longitude, my.lon),\n             latitude = is_within(latitude, my.lat),\n             time = is_within(time, my.period),\n             na.rm = FALSE) %>%\n  # convert time in number of days to a date\n  mutate(date = ymd(\"1950-01-01\") + days(time),\n         year = year(date),\n         month = month(date)) %>%\n  select(-time) -> grid_area_in_time.tb\n\nsum(!is.na(grid_area_in_time.tb$tg))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 43082\n```\n\n\n:::\n\n```{.r .cell-code}\nsum(is.na(grid_area_in_time.tb$tg))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1710518\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(na.omit(grid_area_in_time.tb))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 43,082 × 6\n       tg longitude latitude date        year month\n    <dbl>     <dbl>    <dbl> <date>     <dbl> <dbl>\n 1  0.760     -15.1     64.4 2020-03-02  2020     3\n 2  1.38      -14.9     64.4 2020-03-02  2020     3\n 3 -0.620     -14.9     64.6 2020-03-02  2020     3\n 4  0.870     -14.6     64.6 2020-03-02  2020     3\n 5  1.18      -14.4     64.6 2020-03-02  2020     3\n 6  0.230     -14.4     64.9 2020-03-02  2020     3\n 7  0.920     -14.1     64.9 2020-03-02  2020     3\n 8  1.25      -13.9     64.9 2020-03-02  2020     3\n 9  0.220     -15.1     64.4 2020-03-03  2020     3\n10  0.800     -14.9     64.4 2020-03-03  2020     3\n# ℹ 43,072 more rows\n```\n\n\n:::\n:::\n\n\nFrom here onwards we can continue as shown above for TerraClimate, but replacing the starting date\nfor time of `ymd(\"1900-01-01\")` by `ymd(\"1950-01-01\")` and the missing value\nmarker `32768` by `9999`. These values are taken from the metadata\nwe printed with the code above.\n\n### NASA POWER\n\n[NASA POWER](https://power.larc.nasa.gov) hosts data mainly intended for\nrenewable energy estimates such as temperature, relative humidity,\nprecipitation, solar radiation, clouds, wind speed and wind direction, all in\nall data for 161 parameters. The maximum spatial resolution available is 1/10\ndegree for both latitude and longitude. Geographic coverage is global, hourly\nvalues are available in near-real-time. Gridded values are based on remote\ndata acquired with satellite instruments.  The available solar and weather\ndata can be useful for other purposes in addition to renewable energy\nassessments. These data can be accessed using R package 'nasapower' built on\ntop of the online API. In this case, the data set specific package works\ndifferently, returning a tibble, without exposing NetCDF files to the user.\n\nIn the case of NASA POWER any valid longitude and altitude can be passed, so\nwe do not need to define R code to search for the nearest available grid\npoint as we did above. In addition, the value passed to `community`\nparameter affects the units in which the values are returned. By passing\n`\"AG\"`, for agro-climatology, we obtain the radiation in MJ/m-2/d-1.\nMetadata is also retrieved and stored in the object containing the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nasapower)\n\n# long printout, use to se what is available\n# \n# query_parameters(community = \"ag\",\n#                  temporal_api = \"hourly\")\n\nquery_parameters(par = \"T2M\",\n                 community = \"ag\",\n                 temporal_api = \"hourly\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$T2M\n$T2M$type\n[1] \"METEOROLOGY\"\n\n$T2M$temporal\n[1] \"HOURLY\"\n\n$T2M$source\n[1] \"MERRA2\"\n\n$T2M$community\n[1] \"AG\"\n\n$T2M$calculated\n[1] FALSE\n\n$T2M$inputs\nNULL\n\n$T2M$units\n[1] \"C\"\n\n$T2M$name\n[1] \"Temperature at 2 Meters\"\n\n$T2M$definition\n[1] \"The average air (dry bulb) temperature at 2 meters above the surface of the earth.\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# we download the data for one grid point, Joensuu, Finland\ndaily_single_ag <- get_power(\n  community = \"AG\",\n  lonlat = c(29.76, 62.60),\n  pars = c(\"RH2M\", \"T2M\", \"ALLSKY_SFC_SW_DWN\"),\n  dates = c(\"2020-06-01\", \"2020-06-30\"),\n  temporal_average = \"DAILY\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `temporal_average has been deprecated for `temporal_api`.\nYour query has been modified to use the new terminology for `get_power`.  Please update your scripts to use the new argument.\n```\n\n\n:::\n\n```{.r .cell-code}\n# print the downloaded data\ndaily_single_ag\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNASA/POWER CERES/MERRA2 Native Resolution Daily Data  \n Dates (month/day/year): 06/01/2020 through 06/30/2020  \n Location: Latitude  62.6   Longitude 29.76  \n Elevation from MERRA-2: Average for 0.5 x 0.625 degree lat/lon region = 111.91 meters \n The value for missing source data that cannot be computed or is outside of the sources availability range: NA  \n Parameter(s):  \n \n Parameters: \n RH2M                  MERRA-2 Relative Humidity at 2 Meters (%) ;\n T2M                   MERRA-2 Temperature at 2 Meters (C) ;\n ALLSKY_SFC_SW_DWN     CERES SYN1deg All Sky Surface Shortwave Downward Irradiance (MJ/m^2/day)  \n \n# A tibble: 30 × 10\n     LON   LAT  YEAR    MM    DD   DOY YYYYMMDD    RH2M   T2M ALLSKY_SFC_SW_DWN\n   <dbl> <dbl> <dbl> <int> <int> <int> <date>     <dbl> <dbl>             <dbl>\n 1  29.8  62.6  2020     6     1   153 2020-06-01  76.8  9.94             18.8 \n 2  29.8  62.6  2020     6     2   154 2020-06-02  67.7 12.3              26.2 \n 3  29.8  62.6  2020     6     3   155 2020-06-03  70.2 13.0              26.4 \n 4  29.8  62.6  2020     6     4   156 2020-06-04  81.7 13.0              19.1 \n 5  29.8  62.6  2020     6     5   157 2020-06-05  85.2 14                 9.83\n 6  29.8  62.6  2020     6     6   158 2020-06-06  85.2 14.9              15.4 \n 7  29.8  62.6  2020     6     7   159 2020-06-07  72.6 14.4              24.3 \n 8  29.8  62.6  2020     6     8   160 2020-06-08  89.8 17.2              14.1 \n 9  29.8  62.6  2020     6     9   161 2020-06-09  84.9 17.6              12.5 \n10  29.8  62.6  2020     6    10   162 2020-06-10  85.8 16                22.1 \n# ℹ 20 more rows\n```\n\n\n:::\n:::\n\n\nWe must be aware that near-real-time data from remote\nsensing is not subjected to as thorough a quality control as\nvalidated data made available with a longer delay.\n\n### Erythemal UV radiation and ozone column\n\nWe will import data from a file downloaded manually (using URL https://avdc.gsfc.nasa.gov/index.php?site=164609600&id=79) and corresponding to Ushuaia, Argentina.\nWhen we first download data from a server as a text file, we need to inspect it. Most files will have a header with metadata and which needs to be read separately from the data itself. In this case, opening the file in a text editor, such as RStudio, notepad or nano, we can see that lines from 51 onwards contain the data in tabular shape and that line\n51 contains the variable (= column) names. From this we know that we need to separately read the first 50 lines as a header, and that for reading the data themselves we need to skip 50 lines at the top of the file.\nWe save to the header in the 'comment' attribute of the data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nlibrary(anytime)\nmy.file <- \"data/aura_omi_l2ovp_omuvb_v03_ushuaia.txt\"\nheader <- readLines(my.file, n = 50)\nomiuvb.tb <- read_table(file = my.file,\n                        skip = 50,\n                        col_types = cols(.default = col_double(),\n                                         Datetime = col_character(),\n                                         DOY = col_integer(),\n                                         Orbit = col_character()))\ncomment(omiuvb.tb) <- header\n```\n:::\n\n\nWe will most likely be interested in a few of the columns rather than in all\nof them. We can inspect the header of the file itself, or the documentation\nof the data set to identify the columns containing the data we need, and then\nselect these columns and study the description of these fields, and as shown\nbelow.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# cat(grkmisc::pretty_string(comment(omiuvb.tb), wrap_at = 70, truncate_at = Inf), sep =  \"\\n\")\ncolnames(omiuvb.tb)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"Datetime\"   \"MJD2000\"    \"Year\"       \"DOY\"        \"sec.(UT)\"  \n [6] \"Orbit\"      \"CTP\"        \"Lat.\"       \"Lon.\"       \"Dis\"       \n[11] \"SZA\"        \"VZA\"        \"GPQF\"       \"OMAF\"       \"OMQF\"      \n[16] \"UVBQF\"      \"OMTO3_O3\"   \"CSEDDose\"   \"CSEDRate\"   \"CSIrd305\"  \n[21] \"CSIrd310\"   \"CSIrd324\"   \"CSIrd380\"   \"CldOpt\"     \"EDDose\"    \n[26] \"EDRate\"     \"Ird305\"     \"Ird310\"     \"Ird324\"     \"Ird380\"    \n[31] \"OPEDRate\"   \"OPIrd305\"   \"OPIrd310\"   \"OPIrd324\"   \"OPIrd380\"  \n[36] \"CSUVindex\"  \"OPUVindex\"  \"UVindex\"    \"LambEquRef\" \"SufAlbedo\" \n[41] \"TerrHgt\"   \n```\n\n\n:::\n\n```{.r .cell-code}\ncolumn_selector <-  c(\"Year\", \"DOY\", \"OMTO3_O3\", \"EDDose\", \"UVBQF\", \"OMQF\")\npattern <- paste(column_selector, collapse = \"|\")\nheader[grepl(pattern, header)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Year      : Year\"                                        \n[2] \"DOY       : Day Of Year\"                                 \n[3] \"OMQF      : OMTO3 Quality Flags (dimensionless)\"         \n[4] \"UVBQF     : Quality Flags on Pixel Level (dimensionless)\"\n[5] \"OMTO3_O3  : Total column ozone (DU)\"                     \n[6] \"CSEDDose  : Clear Sky Erythemal Daily Dose (J/m^2)\"      \n[7] \"EDDose    : Erythemal Daily Dose (J/m^2)\"                \n```\n\n\n:::\n\n```{.r .cell-code}\nomiuvb.tb[ , column_selector]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9,508 × 6\n    Year   DOY OMTO3_O3 EDDose UVBQF  OMQF\n   <dbl> <int>    <dbl>  <dbl> <dbl> <dbl>\n 1  2004   275     282.   447. 20480     0\n 2  2004   275     295.   875. 20482 32768\n 3  2004   276     317.  1846. 20480     0\n 4  2004   276     317.  1438. 20480     0\n 5  2004   277     323.  1552. 20482 36864\n 6  2004   277     321.  1340. 20480     0\n 7  2004   278     320.  1631. 20480     0\n 8  2004   278     320.  1650. 20480     0\n 9  2004   279     297.  1385. 20480     0\n10  2004   279     288.  1433. 20482 36864\n# ℹ 9,498 more rows\n```\n\n\n:::\n:::\n\n\nThese data originate from satellite observations and are based on cloudiness\nand aerosols as observed during overpass. Ozone column values are more\nreliable than erythemal UV doses, as ozone column thickness varies very\nlittle between overpasses while cloudiness can vary much more. EDDose can be\nup to 30% overestimated. It is important to read the data product\ndocumentation before using the data and to understand the meaning of the\nquality flags, as some observations may need to be discarded. Estimates are\nspecially unreliable when there is a snow or ice cover on the ground as it\naffects estimates of cloudiness.\n\n\n### Other data sources\n\nThe [European Centre for Medium-Range Weather Forecasts](https://www.ecmwf.int/) \nis another source of useful data. For example the \"ECMWF Reanalysis\nv5\" or ERA5 is a reanalysis of world-wide weather going back all the way to\n1950. These data rely more strongly on modelling but the highest temporal\nresolution available is hourly. The file format used is not NetCDF but,\ninstead, GRIB.\n\nThe [National Oceanic and Atmospheric Administration](https://www.noaa.gov/) (NOAA) in the USA,\nhas for many years provided open access to data. Both data sets with\nnational and world-wide coverage are hosted. The R package 'rNOMADS'\nfacilitates access to these data.\n\nVarious national weather services also make at least some data available\non-line as part of the recent emphasis on \"open government\". For example,\nfor Spain precipitation data with less uncertainty than that in E-OBS is\navailable. In the case of Finland, historical data from meteorological\nstations is one of the data sets available on-line through an API interface\ngiving access to several variables from \n[FMI](https://en.ilmatieteenlaitos.fi/open-data-sets-available). In this\ncase there is a well documented R package which has not yet been published in\nCRAN but that can be installed from a git repository hosted at \n[GitHub](https://github.com/rOpenGov/fmi2).\n\nNASA maintains a metadata catalogue for \n[Earth Observation Data](https://earthdata.nasa.gov/). At this site there is a link [\"Find Data\"](https://www.earthdata.nasa.gov/learn/find-data) allowing searches.\nSearching the catalogue can be overwhelming unless one starts by limiting the\nscope of data sources where to search, say by type of instrument, or some\nother broad criterion.\n\nThe [CEOS International Directory Network (IDN)](https://idn.ceos.org/) can be\nalso of help. For example, searching for \"ozone\nlayer\" and then selecting \"OMI\" as instrument provides a list of useful\ndata sets of manageable length.\n\n[NASA's World View](https://worldview.earthdata.nasa.gov/) makes\nit possible to interactively explore data sets graphically as \"layers\" on a\nworld map, but also provides links to download the same data as numerical\nvalues. Some data are also available through the \n[Google Earth Engine](https://explorer.earthengine.google.com/) after creating\na user account.\n\nThe [UNdata site](https://data.un.org/) gives access to\nworld-wide and country statistics including the data from the [World\nMeteorological Organization](https://public.wmo.int/en).\n\nAlthough off-topic, it is worthwhile mentioning that other types of data\nuseful in our research are also becoming available on-line. These data sets include several related to species distributions, phenology and other data relevant to ecological research. In many cases,\nspecific R packages are available for retrieval and import of the data.\nFor R users it is worthwhile keeping a close eye on the \n[ROpenScience web site](https://ropensci.org/) and on their [catalogue of peer-reviewed R packages](https://ropensci.org/packages/data-access/).\n\n\\section*{Additional thoughts}\n\nFew researchers in our field, including myself, extract all the information\ncontained in the data they collect. Fewer, extract all the information\nobtainable by combining the data they collect with data about the context in\nwhich they do research. Even fewer quantitatively combine data over long\nperiods or time, across research groups, institutions, countries, and\ncontinents, etc. In contrast, meteorologists do it all the time: they extract\ninformation, about very slow rates of change with background variation that\nis comparatively huge. How do they achieve this? Extreme care about methods:\ncalibrations, cross-calibrations, and all sorts validations, cross\nvalidations and inter-comparisons. What does this mean in practice? With\ncareful computations and further validation they can produce long time series\nof data covering most of our planet, based on measurements done by many\nthousands, possibly even millions, of different observers, sometimes using\ndifferent methods, over hundreds of years! This is the source of the data we\nhave now in our hands, for free. And how valuable this is!\n\nI see this as the most cogent demonstration of what can be achieved when most\nresearchers or ``data collectors'' in a field aim consistently over a long\ntime at what we now call ``reproducible research''. In biology, we can seldom\neven get compatible results from a repeat of a single experiment. The\ncomplexity of biological systems is no excuse for this, as long as we do not\neven try to have enough replicates, document methods in enough detail and\nspend enough time and effort validating and calibrating methods against each\nother. I think, it is good time for biological research to catchup and follow\nthe meteorologists' example. To achieve this, we need to get researchers in\nour field to think about the long term value of the data we produce and how\nto enhance it. This is made difficult, when the evaluation of our\nproductivity is so often based on the most recent 5 years and on the\nnewsworthiness of individual papers rather than on long-term impact and\nreliability of experimental design and methods.\n\nThe \\emph{World Meteorological Organization} has encouraged open sharing of\ndata and consistency of methods since its creation as the _International\nMeteorological Organization_ in 1873. In my opinion, this demonstrates that\nopen availability of original data, allowing comparison and cross-checking\ncan be a strong incentive for adoption of reproducible research approaches.\nAs demonstrated by meteorological observations, the value added by generating\ndata and its metadata in ways that allows the build up of large sets of\ndata by combining a multitude of individual sources can be enormous. Just,\nhow could we have dealt or even detected global climate change lacking\nconsistently collected data from well before we realized that climate is\nchanging?\n\nConsidering the shorter time frame of the current pandemic, that useful data\nare becoming so easily available opens an opportunity for keeping us busy. We\ncan extract new information from these freely available data sets and also by\ncombining them with the data we have earlier collected ourselves. So even if\nwe are confined at home because of the COVID epidemic we can do original\nresearch!\n\n## Related reading\n\nA paper just published discusses satellite-retrieved UV-A data and its importance for biological research [@Parisi2021]. Another paper, available only as a preprint, describes real-time UV-Index retrieval at high spatial and temporal resolution over the whole of Europe [@Kosmopoulos2021]. An earlier paper is also of interest as it discusses some of the sources of bias in UV-irradiance retrieved from satellites [@Arola2009]. Finally an agronomic paper of interest in relation to how biases in weather data retrieved from satellite observations can affect estimates of biological responses [@Bai2010].\n\nChapter 8 in my book _Learn R: As a Language_[@Aphalo2020a] provides code examples and explanations for importing data from files using additional formats to those I have discussed here.\n\n## Acknowledgements\n\nI warmly thank the discussions and thoughtful comments from Titta Kotilainen\nthat helped me write and revise this column. I also thank Anders Lindfors for\nintroducing me to the use of some of these sources of information and Heikki\nHänninen for sharing a manuscript from his research group, a manuscript that\nwoke up my curiosity about open data even more. This last step made the\n\"energy level\" surpass my activation threshold and got me playing with\nthese databases, updating an old computer and downloading many gigabytes of\nclimate, weather, phenological and other data. What started as tinkering\nseveral weeks ago, ended providing useful context data for three manuscripts\nthat I am working on at the moment, an ideas for a couple of future articles!\nNot bad at all, and so an experience worthwhile sharing with our readers.\n\n## References\n\n::: {#refs}\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}