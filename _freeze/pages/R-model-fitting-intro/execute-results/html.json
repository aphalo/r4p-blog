{
  "hash": "2c9824b7e3c156d2ae2fac8e37e660f6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model fitting in R\"\nsubtitle: \"An introduction to `lm()`, `glm()` and `nls()`\"\nauthor: \"Pedro J. Aphalo\"\ndate: 2023-05-30\ndate-modified: 2023-11-27\ncategories: [R, model fitting]\nkeywords: [predicted values, residual values, parameter estimates, model formulas]\nformat:\n  html:\n    code-fold: true\n    code-tools: true\nabstract: \n  In this page I give a brief presentation on the mechanics of fitting statistical models to observed data using R. I use a lineal model (LM), polynomial regression, as example. I describe the most important values contained in the model fit object returned by function `lm()` and how to extract them. I rely heavily on diagrams and data plots.\n---\n\n\n# Introduction\n\n_Fitting a model_ to data consists in finding the parameter values that best\nexplain the data or observations. These parameter values are estimates of\nthose in a larger population of possible observations from which we have\ndrawn a sample or subset. For deciding what is best, we need a criterion, and\nthe most frequently used one is minimizing the \"residual variance\" by the\nmethod of ordinary least squares (OLS).\n\n_Model selection_ involves comparing models that differ in their structure, i.e., \nin the formula that relates parameters and data. Here we also need a criterion\nto \"measure\" how well a model describes the data, for example, AIC or BIC,\nwhich are information criteria. Alternatively, we can just choose \"the simplest\nmodel that does a good enough job\".\n\nDifferent approaches to model fitting make different assumptions about the \nobservations. What assumptions are reasonable for our data determines what \nmethods are applicable. Out of these, we can choose the one that best suits\nthe nature of our scientific or practical question or hypothesis.\n\nModern statistical algorithms, and their implementation in R, strive to be\nwidely applicable. I will emphasize this as well as what is common to different\nmethods.\n\nIn general, there are many different aspects of a model fit that we may be\ninterested in. The most computationally intensive step is the fitting itself.\nThus R's approach is to save the results from fitting, or _fitted model_, and\nseparately query it for different derived numerical and graphical output as \nneeded.\n\n::: callout-tip\nIn this page some code chunks are \"folded\" so as to decrease the clutter. Above the R\noutput, text or plots, you will find a small triangle followed by \"Code\".\nClicking on the triangle \"unfolds\" the code chunk making visible the R code used\nto produce the output shown.\nThe code in the chunks can be copied by clicking on the top right corner, where\nan icon appears when the mouse cursor hovers over the code listing.\n:::\n\nAttach packages used for plotting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggpmisc)\nlibrary(broom)\ntheme_set(theme_bw(16))\n```\n:::\n\n\n# Linear models (LM)\n\n## Fitting\n\nI use here polynomial regressions as an example.\n\nGenerate some artificial data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(19065)\n# set.seed(4321) # change or comment out to get different pseudorandom values\nx <- 1:24\ny <- (x + x^2 + x^3) + rnorm(length(x), mean = 0, sd = mean(x^3) / 2)\ny <- y / max(y)\nmy.data <- data.frame(x, \n                      y, \n                      group = c(\"A\", \"B\"), \n                      y2 = y * c(1, 2) + c(0, 0.2),\n                      block = c(\"a\", \"a\", \"b\", \"b\"),\n                      wt = sqrt(x))\n```\n:::\n\n\nPlot these data to have a look at them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(my.data, aes(x, y)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](R-model-fitting-intro_files/figure-html/unnamed-chunk-3-1.svg)\n:::\n:::\n\n\nIn R we first save the fitted model object into a variable, and in a second stage extract or query different results as needed.\n\n\n```{mermaid}\n%%| label: fig-barebones\n%%| fig-cap: A minimalist diagram of model fitting in R.\n%%{init: {\"htmlLabels\": true} }%%\n\nflowchart LR\n  A(<i>model formula</i>) --> B[model fit\\nfunction] --> C(model fit\\nobject) --> D1['diagnostics' plots]\n  AA(<i>observations</i>) --> B\n  C --> D2[query methods]\n```\n\n\nHere we fit a third-degree polynomial regression using function `lm()` (linear model, LM). .\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nfm1 <- lm(formula = y ~ poly(x, 3), data = my.data)\n```\n:::\n\n\nThen we query this object saved in the variable, here `fm1`, with different methods, \nfor example `summary()`.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nsummary(fm1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ poly(x, 3), data = my.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.24817 -0.07635  0.01945  0.08890  0.18264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.25644    0.02525  10.154 2.44e-09 ***\npoly(x, 3)1  1.49842    0.12372  12.111 1.15e-10 ***\npoly(x, 3)2  0.55768    0.12372   4.508 0.000215 ***\npoly(x, 3)3 -0.06142    0.12372  -0.496 0.625015    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1237 on 20 degrees of freedom\nMultiple R-squared:  0.8932,\tAdjusted R-squared:  0.8772 \nF-statistic: 55.75 on 3 and 20 DF,  p-value: 6.793e-10\n```\n\n\n:::\n:::\n\n\n## The components of the model fit object\n\nFunctions to extract and/or compute different estimates and carry out tests are available. These functions take the model fit object as argument. Above I showed the use of `summary()`. The next diagram shows most of the functions that can be used with linear-model-fit objects.\n \n\n```{mermaid}\n%%| label: fig-lm\n%%| fig-cap: A diagram of linear-model (LM) fitting in R.\n%%{init: {\"htmlLabels\": true} }%%\n\nflowchart LR\n  A1(<i>model formula</i>) --> B[\"<code>lm()</code>\"] --> C(<code>lm</code> object) --> C1[\"<code>plot()</code>\"]\n  A2(<i>observations</i>) --> B\n  A3(<i>weights</i>) -.-> B\n  C --> C2[\"<code>summary()</code>\"]\nC --> C3[\"<code>anova()</code>\"]\nC --> C4[\"<code>residuals()</code>\"]\nC --> C5[\"<code>fitted()</code>\"]\nC --> C6[\"<code>AIC()</code>\"]\nC --> C7[\"<code>BIC()</code>\"]\nC --> C8[\"<code>coefficients()</code>\"]\nC --> C11[\"<code>formula()</code>\"]\nC --> C12[\"<code>weights()</code>\"]\nC --> C9[\"<code>confint()</code>\"]\nC --> C10[\"<code>predict()</code>\"]\nBB(\"<i>new data</i>\") --> C10\n```\n\n\nI expect you to be already (to some extent) familiar with the ANOVA table, coefficient estimates and plots of residuals. But, you could be still wordering about how does all this fit together? I will use plots to explain the most interesting estimates and results from model fitting. R's functions in the diagram above, return numeric values. Thus, for creating the plots, I use these functions indirectly through functions from package 'ggpmisc'. This mainly saves typing. The model formula, data and model fit function used for the plots below, are the same as in the example above.\n\nThe observations (artificial data in this case). Same plot as above, will be the base of subsequent plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(my.data, aes(x = x, y = y)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](R-model-fitting-intro_files/figure-html/unnamed-chunk-8-1.svg)\n:::\n:::\n\n\nThe _fitted values_, as blue points, added. Fitted values are the \"model predictions\" for the observed values of _x_.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(my.data, aes(x = x, y = y)) +\n  geom_point() +\n  stat_fit_fitted(formula = y ~ poly(x, 3), \n                  colour = \"blue\")\n```\n\n::: {.cell-output-display}\n![](R-model-fitting-intro_files/figure-html/unnamed-chunk-9-1.svg)\n:::\n:::\n\n\nThe residuals plotted as deviations from the fitted values. These residuals (in red) are used as the criterion for finding the best fit. The observations (in black) are given, but the position of the predicted values (in blue) are decided based on the minimization algorithm used as the criterion to fit a model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(my.data, aes(x = x, y = y)) +\n  geom_point() +\n  stat_fit_deviations(formula = y ~ poly(x, 3), \n                      colour = \"red\", \n                      arrow = arrow(length = unit(0.33, \"lines\"), ends = \"both\")) +\n  stat_fit_fitted(formula = y ~ poly(x, 3), colour = \"blue\")\n```\n\n::: {.cell-output-display}\n![](R-model-fitting-intro_files/figure-html/unnamed-chunk-10-1.svg)\n:::\n:::\n\n\n\nThe residuals plotted on their own. Plots like the one below can be used mainly as a tool to check that the assumptions, in our case those required to fit a linear model using least squares as a criterion are fulfilled, at least approximately (homogeneity of variace, in particular). In this plot the values on the _y_ axis describe the length of red segments in the plot above and are shown as red points for each observed value of _x_.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(my.data, aes(x = x, y = y)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  stat_fit_residuals(formula = y ~ poly(x, 3),\n                     colour = \"red\")\n```\n\n::: {.cell-output-display}\n![](R-model-fitting-intro_files/figure-html/unnamed-chunk-11-1.svg)\n:::\n:::\n\n\nThe form of the _equation of the fitted curve_ is given by the model formula passed as argument to `lm()`, but the estimates of the value of the four parameters in the equation are those giving the curve that makes the sum of squares of of the residuals as small as possible.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(my.data, aes(x = x, y = y)) +\n  geom_point() +\n  stat_fit_deviations(formula = y ~ poly(x, 3), \n                      colour = \"red\", \n                      arrow = arrow(length = unit(0.33, \"lines\"), ends = \"both\")) +\n  stat_fit_fitted(formula = y ~ poly(x, 3), colour = \"blue\") +\n  stat_poly_eq(formula = y ~ poly(x, 3),\n               mapping = use_label(\"eq\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in check_poly_formula(formula, orientation): 'poly()' in model formula\nhas to be passed 'raw = TRUE'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](R-model-fitting-intro_files/figure-html/unnamed-chunk-12-1.svg)\n:::\n:::\n\n\nThe _equation_ describes a curve. The prediction line passes through the fitted values but interpolates between them as shown immediately below, and, as I will show farther below, can extrapolate outside the range of the observed _x_ values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(my.data, aes(x = x, y = y)) +\n  geom_point() +\n  stat_poly_line(formula = y ~ poly(x, 3), se = FALSE) +\n  stat_poly_eq(formula = y ~ poly(x, 3),\n               mapping = use_label(\"eq\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in check_poly_formula(formula, orientation): 'poly()' in model formula\nhas to be passed 'raw = TRUE'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](R-model-fitting-intro_files/figure-html/unnamed-chunk-13-1.svg)\n:::\n:::\n\n\nThe prediction line and $R^2$, $n$, $F\\textrm{-value}$ and $P\\textrm{-value}$. We include as annotations other parameter estimates of interest.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(my.data, aes(x = x, y = y)) +\n  geom_point() +\n  stat_poly_line(formula = y ~ poly(x, 3), se = FALSE) +\n  stat_poly_eq(formula = y ~ poly(x, 3),\n               mapping = use_label(\"eq\")) +\n  stat_poly_eq(formula = y ~ poly(x, 3),\n               label.y = 0.89,\n               mapping = use_label(c(\"R2\", \"n\", \"F\", \"P\")))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in check_poly_formula(formula, orientation): 'poly()' in model formula\nhas to be passed 'raw = TRUE'\nWarning in check_poly_formula(formula, orientation): 'poly()' in model formula\nhas to be passed 'raw = TRUE'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](R-model-fitting-intro_files/figure-html/unnamed-chunk-14-1.svg)\n:::\n:::\n\n\nWhen additional details for each parameter estimate are of interest, a table is usually used. The _b_~i~ in the table are the coefficients in the equation in the plot above, and the _s_~i~ are the their corresponding standard errors, from which a _t_-value can be computed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(my.data, aes(x = x, y = y)) +\n  geom_point() +\n  stat_poly_line(formula = y ~ poly(x, 3), se = FALSE) +\n  stat_fit_tb(method.args = list(formula = y ~ poly(x, 3)),\n              tb.vars = c(Term = 1, \n                          \"italic(b)[i]\" = 2,\n                          \"italic(s)[i]\" = 3, \n                          \"italic(b)[i] / italic(s)[i]~`=`~italic(t)\" = 4,\n                          \"italic(P)\" = 5),\n              tb.params = c(\"x^0\" = 1,\n                            \"x^1\" = 2,\n                            \"x^2\" = 3,\n                            \"x^3\" = 4),\n              parse = TRUE,\n              label.x = \"left\")\n```\n\n::: {.cell-output-display}\n![](R-model-fitting-intro_files/figure-html/unnamed-chunk-15-1.svg)\n:::\n:::\n\n\nUsing the _b_~i~ together with the _s_~i~, we can compute an estimate of the variation affecting the line as a whole. This makes possible the computation of a confidence band around the predicted curve. The prediction line plus its 95% confidence band are shown in the plot below. This band informs us about where the true line (for the sampled population as a whole) is likely to reside. The caveat is, that this estimated band and curve are true only given the form of the model that was used. If the form of model we have passed as argument can take a suitable shape and is flexible enough to describe features of interest the prediction and band are useful, otherwise they are of little use. This we can best judge by looking at a plot like the one below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(my.data, aes(x = x, y = y)) +\n         geom_point() +\n  stat_poly_line(formula = y ~ poly(x, 3), se = TRUE)\n```\n\n::: {.cell-output-display}\n![](R-model-fitting-intro_files/figure-html/unnamed-chunk-16-1.svg)\n:::\n:::\n\n\nThe residuals plotted as deviations from the prediction line. This is an alternative way of subjectively assessing the goodness of a model fit. Be aware, that how many observations fall outside the band depends on the number of replicates: the more observations we have, the more confident we can be about the estimated line, and the higher the proportion of observations that will be plotted outside the band. \nWhat we have plotted is a confidence band for the curve itself, not a density estimate for the probability of observing _x_ and _y_ value pairs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(my.data, aes(x = x, y = y)) +\n  geom_point() +\n  stat_poly_line(formula = y ~ poly(x, 3), se = TRUE) +\n  stat_fit_deviations(formula = y ~ poly(x, 3), colour = \"red\", \n                      arrow = arrow(length = unit(0.33, \"lines\"), ends = \"both\"))\n```\n\n::: {.cell-output-display}\n![](R-model-fitting-intro_files/figure-html/unnamed-chunk-17-1.svg)\n:::\n:::\n\n\nThe prediction line plus its 95% confidence band with extrapolation. When we extrapolate into \"unknown territory\" uncertainties rapidly increase and the band widens dramatically, and in a way that depends strongly on the model formula. (The range of the _y_ axis is expanded!)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(my.data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_vline(xintercept = range(my.data$x), linewidth = 0.33) +\n  stat_poly_line(formula = y ~ poly(x, 3), se = TRUE, fullrange = TRUE) +\n  expand_limits(x = c(-10, 40)) # an arbitrary range\n```\n\n::: {.cell-output-display}\n![](R-model-fitting-intro_files/figure-html/unnamed-chunk-18-1.svg)\n:::\n:::\n\n\n# Generalised linear models (GLM)\n\nAs shown in the diagram below, the overall approach is very similar to that used for linear models.\n\n\n```{mermaid}\n%%| label: fig-glm\n%%| fig-cap: A diagram of generalized-linear-model (GLM) fitting in R. Query methods as in @fig-lm.\n%%{init: {\"htmlLabels\": true} }%%\n\nflowchart LR\n  A1(<i>model formula</i>) --> B[\"<code>glm()</code>\"] --> C(<code>glm</code> object) --> C1[query methods]\n  A2(<i>observations</i>) --> B\n  A3(<i>weights</i>) -.-> B\n  A4(<i>family</i> and <i>link</i>) --> B\n```\n\n\n# Non-linear models\n\nAs shown in the diagram below, the overall approach is very similar to that used for linear models.\n\n\n```{mermaid}\n%%| label: fig-nls\n%%| fig-cap: A diagram of nonlinear least squares (NLS) model fitting by numerical approximation in R. Query methods similar to those in @fig-lm.\n%%{init: {\"htmlLabels\": true} }%%\n\nflowchart LR\n  A1(<i>model formula</i>) --> B[\"<code>nls()</code>\"] --> C(<code>nls</code> object) --> C1[query methods]\n  A2(<i>observations</i>) --> B\n  A3(<i>weights</i>) -.-> B\n  A5(<i>starting values</i>) --> B\n```\n\n\n# Model selection flowchart\n\nI mentioned above that the model chosen was given by the argument we passed to `lm()`. Obviously, it is also possible to use other models. The choice of a model has to balance increasing the proportion of the total variation that is explained while still extracting the useful information out of the data separately from accounting for the noise. One approach is model selection, instead of simply finding the best set of parameter estimates, comparing the best fits possible with each equation form, and selecting the best performing one.\n\nModel selection can be done manually by comparing models fitted individually or automatically using a stepwise approach. In the case of polynomials, one possibility is to compare polynomials of different degrees. Automatic selection is described by the diagram below.\n\n\n```{mermaid}\n%%| label: fig-lm-step\n%%| fig-cap: A diagram of linear-model (LM) fitting with stepwise model selection in R.\n%%{init: {\"htmlLabels\": true} }%%\n\nflowchart TB\n  A1(<i>model formula</i>) --> B[\"<code>lm()</code>\"] --> C(<code>lm</code> object)\n  A2(<i>observations</i>) --> B\n  A3(<i>weights</i>) -.-> B\n  C --> CI[query methods]\n  C --> C1[\"<code>step()</code>\"]\n  subgraph Z [\"<strong>Model selection</strong>\"]\n  C1 --> C3(<code>lm</code>  object)\n  z1(most complex\\n<i>model formula</i>) -.-> C1\n  z2(simplest nested\\n<i>model formula</i>) -.-> C1\n  C3 --> CF[query methods]\n  end\n  style Z fill:#fff\n```\n\n\n::: callout-warning\nThe assumption of most usual model fitting procedures is that residuals are normally distributed and independent of the magnitude of the response variable, not that the observations themselves are normally distributed. Only in the simplest cases, such as comparing a mean against a constant, the residuals have the same distribution as the data but centred on zero instead of on the mean. In most other cases, this is not the case, as part of the variation among individual observations is accounted by terms in the fitted model, such as random effects, correlations or even variance covariates. The justification is that we use the residuals to estimate the _error_ variation and the tests of significance are based on this error variance estimate.\n\nIt is also of little use to test for the statistical significance of the deviations from these assumptions, as we should not expect in the real world for the assumptions to be ever exactly fulfilled. The power of tests increases with replication, but the bias introduced into estimates does not depend on significance, but on the magnitude of the deviations. Furthermore, the higher the replication, the less the bias introduced in the estimates by deviations from the assumptions. In other words, the more replicates we have smaller deviations from assumptions become detectable as significant, while the importance of deviations decreases.\n\nWhen there are very few replicates available, it is imposible to assess directly if observations come from a normally distributed population or not. In such cases, it can be wise to rely on previous information about the sampled population and sampling method used instead of on the current observations.\n:::\n\n::: callout-tip\nTo create the plots above I used packages 'ggplot2' and 'ggpmisc'. [Galleries\nof plot examples using 'ggpmisc' and some other\npackages](https://www.r4photobiology.info/galleries.html) are available at the\n[R for Photobiology web site](https://www.r4photobiology.info). They contain R\ncode folded in the same way as in this page.\n:::\n",
    "supporting": [
      "R-model-fitting-intro_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}