{
  "hash": "88588f17eb69b1c38fbc4117ecedce0b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear Models (DRAFT)\"\nsubtitle: \"An introduction using `lm()`\"\nauthor: \"Pedro J. Aphalo\"\ndate: 2023-11-27\ndate-modified: 2023-11-27\ncategories: [R, model fitting]\nkeywords: [predicted values, residual values, parameter estimates, model formulas]\nformat:\n  html:\n    code-fold: true\n    code-tools: true\nabstract: \n  In this page I give a brief presentation on the mechanics of fitting statistical models to observed data using R. I use a lineal model (LM), polynomial regression, as example. I describe the most important values contained in the model fit object returned by function `lm()` and how to extract them. I rely heavily on diagrams and data plots.\n---\n\n\n## Introduction\n\n_Fitting a model_ to data consists in finding the parameter values that best\nexplain the data or observations. These parameter values are estimates of\nthose in a larger population of possible observations from which we have\ndrawn a sample or subset. For deciding what is best, we need a criterion, and\nthe most frequently used one is minimizing the \"residual variance\" by the\nmethod of ordinary least squares (OLS).\n\n_Model selection_ involves comparing models that differ in their structure, i.e., \nin the formula that relates parameters and data. Here we also need a criterion\nto \"measure\" how well a model describes the data, for example, AIC or BIC,\nwhich are information criteria. Alternatively, we can just choose \"the simplest\nmodel that does a good enough job\".\n\nDifferent approaches to model fitting make different assumptions about the \nobservations. What assumptions are reasonable for our data determines what \nmethods are applicable. Out of these, we can choose the one that best suits\nthe nature of our scientific or practical question or hypothesis.\n\nModern statistical algorithms, and their implementation in R, strive to be\nwidely applicable. I will emphasize this as well as what is common to different\nmethods.\n\nIn general, there are many different aspects of a model fit that we may be\ninterested in. The most computationally intensive step is the fitting itself.\nThus R's approach is to save the results from fitting, or _fitted model_, and\nseparately query it for different derived numerical and graphical output as \nneeded.\n\n::: callout-tip\nIn this page some code chunks are \"folded\" so as to decrease the clutter. Above the R\noutput, text or plots, you will find a small triangle followed by \"Code\".\nClicking on the triangle \"unfolds\" the code chunk making visible the R code used\nto produce the output shown.\nThe code in the chunks can be copied by clicking on the top right corner, where\nan icon appears when the mouse cursor hovers over the code listing.\n:::\n\nAttach packages used for plotting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggpmisc)\nlibrary(ggbeeswarm)\nlibrary(broom)\ntheme_set(theme_bw(16))\n```\n:::\n\n\n::: callout-caution\n# Anscombe's linear regression examples\n\nThis classical example from Anscombe (1973) demonstrates four very different data sets that yield exactly the same results when a linear regression model is fit to them, including $R^2$ and $P$ values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# we rearrange the data\nmy.mat <- matrix(as.matrix(anscombe), ncol=2)\nmy.anscombe <- \n  data.frame(x = my.mat[ , 1],\n             y = my.mat[ , 2],\n             case=factor(rep(1:4, rep(11,4))))\nmy.formula = y ~ x\nggplot(my.anscombe, aes(x,y)) +\n  geom_point(shape=21, fill=\"orange\", size=3) +\n  geom_smooth(method=\"lm\", formula = my.formula) +\n  stat_poly_eq(formula = my.formula, \n               parse = TRUE, \n               label.y = \"top\", \n               label.x = \"left\", \n               use_label(c(\"eq\", \"R2\", \"P\"))) +\n  facet_wrap(~case, ncol=2) +\n  theme_bw(16)\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-2-1.svg)\n:::\n:::\n\n  1.  If these four sets of data were real observations from an experiment, would you conclude the same in all cases with respect to the effect of $x$ on the observed $y$?\n  1.  In which of the four cases the estimated line best described the _overall trend_ of the response?\n  1-  What is the difficulty in each of the other three cases?\n\nThese concocted data examples demonstrates that when fitting any model to data, extreme caution is always needed. **We need to check graphically the observations and how well model estimates fit them.** $R^2$, $P$ values, and other parameter's estimates _must always be considered in the light of the data, rather than in isolation_.\n:::\n\n## Linear models (LM)\n\nI will start with some \"unusual examples\" (crazy approaches?) to try to convince you that there is much in common between model fitting and the simple statistics you already know. Grasping this is important as general concepts allows us to understand the bigger picture, and thus we can learn new methods as variations of, or extensions to, what we already know.\n\n### The mean (= average) as a fitted linear model\n\nA vector of 20 random numbers to play with.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmy.vector <- rnorm(20)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmean(my.vector)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.1032552\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nvar(my.vector)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9336702\n```\n\n\n:::\n:::\n\nWe fit a linear model, with only an intercept.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nfm1 <- lm(my.vector ~ 1)\nsummary(fm1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = my.vector ~ 1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.79782 -0.54415 -0.07868  0.46746  1.91060 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  -0.1033     0.2161  -0.478    0.638\n\nResidual standard error: 0.9663 on 19 degrees of freedom\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nanova(fm1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: my.vector\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals 19  17.74 0.93367               \n```\n\n\n:::\n:::\n\n\nTo fit the mean as a linear model, the \"Mean Sq\", or MS, which is just the same variance computed above was minimized.\n\nSo fitting a model is in fact similar to computing the mean of a single variable. However, while the mean is a single parameter, we can fit models with several parameters which are adjusted simultaneously to fit a point, curve, surface or hyper-surface, depending on the case.\n\n### The _t_-test as a fitted model linear model\n\nTwo vectors of numbers to play with.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nvec1 <- 0 + rnorm(20)\nvec2 <- 1 + rnorm(20)\n```\n:::\n\n\nWe can assemble a `data.frame` with them.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ndf1 <- data.frame(obs = c(vec1, vec2), group = rep(c(\"g1\", \"g2\"), each = 20))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df1, aes(group, obs)) +\n  stat_summary(fun.data = mean_se, color = \"blue\") +\n  geom_beeswarm()\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-8-1.svg)\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmean(vec1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8012056\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nmean(vec2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.055059\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nsd(vec1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.098805\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nsd(vec2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9957644\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nt.test(vec1, vec2, var.equal = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  vec1 and vec2\nt = -0.76559, df = 38, p-value = 0.4487\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.9251042  0.4173970\nsample estimates:\nmean of x mean of y \n0.8012056 1.0550592 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nt.test(obs ~ group, data = df1, var.equal = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  obs by group\nt = -0.76559, df = 38, p-value = 0.4487\nalternative hypothesis: true difference in means between group g1 and group g2 is not equal to 0\n95 percent confidence interval:\n -0.9251042  0.4173970\nsample estimates:\nmean in group g1 mean in group g2 \n       0.8012056        1.0550592 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nfm2 <- lm(obs ~ group, data = df1)\nsummary(fm2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = obs ~ group, data = df1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.95731 -0.79944  0.09362  0.75885  2.28499 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   0.8012     0.2345   3.417  0.00152 **\ngroupg2       0.2539     0.3316   0.766  0.44865   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.049 on 38 degrees of freedom\nMultiple R-squared:  0.01519,\tAdjusted R-squared:  -0.01073 \nF-statistic: 0.5861 on 1 and 38 DF,  p-value: 0.4487\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nanova(fm2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: obs\n          Df Sum Sq Mean Sq F value Pr(>F)\ngroup      1  0.644 0.64442  0.5861 0.4487\nResiduals 38 41.779 1.09946               \n```\n\n\n:::\n:::\n\n\nSo, these two examples above show that model fitting provides estimates of population parameters, identically to computing a mean, as well as serving as the basis for tests of significance identical to the _t_-test. In addition, as we will see below, linear models set up more complex tests of significance, as well as fit models that require the simultaneous estimation of multiple parameters.\n\n### ANOVA, regression and linear models\n\nLinear models include what is called Analysis of Variance (ANOVA), linear regression, multiple regression, Analysis of Covariance, and a few other variations all wrapped together. So, one procedure that caters for a wide range of situations and designs of experiments.\n\nLinear models can be computed very efficiently but make several assumptions about the data. Normally distributed and homogeneous variation of residuals.\n\n## ANOVA vs. Regression\n\nIn one-way ANOVA for a completely randomized design we have the\nfollowing model: every population mean contains $\\mu$, whereas the\n$i$th treatment contains $\\tau_i$ but no other $\\tau$.\n\nThe observations $y_{ij}$ follow the linear model\n$$\ny_{ij} = \\mu + \\tau_i + \\epsilon_{ij},\n$$\nwhere $\\mu$ is the population mean, $\\tau_i$ the effect of\ntreatment $i$, and $\\epsilon_{ij}$ the random variation associated\nwith observation $y_{ij}$.\n\nWe assume that the residuals $\\epsilon_{ij}$\nare independent and equally distributed --- $\\epsilon_{ij} \\sim\nN(0,\\sigma^2)$, where variance $\\sigma^2$ is unknown and\nindependent of group $i$.\\vspace{2cm}\n\n---\n\nIn linear regression the $y$ values are obtained from several\npopulations, each population being determined by the corresponding\n$x$ value. The $y$ variable is called _dependent variable_\nand the $x$ variable is called _independent variable_.\n\nThe observations $y_{ij}$ follow the linear model\n$$\ny_{ij} = \\alpha + \\beta x_i + \\epsilon_{ij},\n$$\n\nwhere $\\alpha$ is the population mean when $x=0$, $x_i$ are the\nobserved values for the independent variable, $\\beta$ is the\ncoefficient describing the slope, and $\\epsilon_{ij}$ the random\nvariation associated with observation $y_{ij}$.\n\nWe also assume that the residuals\n$\\epsilon_{ij}$ are independent and equally distributed ---\n$\\epsilon_{ij} \\sim N(0,\\sigma^2)$, where variance $\\sigma^2$ is\nunknown and independent of $x$.\n\nIt is assumed that the $x$ is measured without error.\n\nRegression provides a means of predicting $y$ from $x$, but not the reverse.\n\n## Linear models in R\n\nBoth **ANOVA** and **regression** are described by **linear models** and have\nmuch more in common than what it looks at first sight.\n\nIn regression analysis _dummy_ variables can be used to code\ntreatments.\n\nIn R little distinction is made between ANOVA and regression.\n\nThe function`lm()` can be used for both, whether the variables\nin the model are `factor`s or `numeric` vectors\ndetermines the coding (of the matrix used for describing the tests).\n\nThe function `aov()` uses `lm()` internally but differs in\nthat summary gives an ANOVA table, and in that it can also deal\ndirectly with hierarchical models (e.g. split-plot designs).\n\n## Examples in R\n\n\n\n### Interpretation of the intercept\n\nWe generate some artificial data (it will be different each time you run the code below.)\n\nWhy will it be different? Try it?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 100:120\ny <- x * 0.02 + rnorm(21, sd = 0.05)\nmy.data <- data.frame(x, y)\n```\n:::\n\n\nThe we fit a model, based on the tests in the code chunk below, will in be an ANOVA or a regression? \n\n\n::: {.cell}\n\n```{.r .cell-code}\nis.factor(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n\n```{.r .cell-code}\nis.numeric(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\nis.factor(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n\n```{.r .cell-code}\nis.numeric(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmf1 <- lm(y ~ x, data = my.data)\nanova(mf1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: y\n          Df   Sum Sq Mean Sq F value    Pr(>F)    \nx          1 0.258251 0.25825  189.85 2.428e-11 ***\nResiduals 19 0.025846 0.00136                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(mf1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x, data = my.data)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.073724 -0.018908 -0.005719  0.021752  0.067834 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.193247   0.146428    1.32    0.203    \nx           0.018314   0.001329   13.78 2.43e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03688 on 19 degrees of freedom\nMultiple R-squared:  0.909,\tAdjusted R-squared:  0.9042 \nF-statistic: 189.8 on 1 and 19 DF,  p-value: 2.428e-11\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(y ~ x, data = my.data)\nabline(mf1)\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-15-1.svg)\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmf2 <- lm(y ~ I(x - 100), data = my.data)\nanova(mf2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: y\n           Df   Sum Sq Mean Sq F value    Pr(>F)    \nI(x - 100)  1 0.258251 0.25825  189.85 2.428e-11 ***\nResiduals  19 0.025846 0.00136                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(mf2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ I(x - 100), data = my.data)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.073724 -0.018908 -0.005719  0.021752  0.067834 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 2.024616   0.015538  130.30  < 2e-16 ***\nI(x - 100)  0.018314   0.001329   13.78 2.43e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03688 on 19 degrees of freedom\nMultiple R-squared:  0.909,\tAdjusted R-squared:  0.9042 \nF-statistic: 189.8 on 1 and 19 DF,  p-value: 2.428e-11\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(y ~ I(x - 100), data = my.data)\nabline(mf2)\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-16-1.svg)\n:::\n:::\n\n\n### Residuals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mf1, which = 1)\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-17-1.svg)\n:::\n\n```{.r .cell-code}\nplot(mf1, which = 2)\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-17-2.svg)\n:::\n:::\n\n\nIn _regression_ the assumption is that the explanatory variable is **measured** without error.\n\nWhat does this mean? Does it mean that the population of _x_ has no variation? or something else?\n\n### Regression of _x_ on _y_\n\nIf we switch _X_ and _y_ we now assume that _y_s are **measured** without error.\n\n### Regression and change points\n\nWe will use a modified dataset, based on the pH data for rivers of\nthe South Island of New Zealand.\n\nFirst means for each month were calculated from the five\nobservations in each random sample.\n\nThen a new variable was created by adding:\n`c(rep(0, 60), (1:60) / 40)` to the vector\nholding the mean pH values.\n\nThis is a linear trend starting at the beginning of the third year\nof observations and having an increase in pH of 0.05 units per\nmonth.\n\n### Data used in examples\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lubridate)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'lubridate'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n```\n\n\n:::\n\n```{.r .cell-code}\nload(\"data-files/SI.long.rda\")\nSI.data <- data.frame(date = ymd(\"1980-01-01\", tz = \"UTC\") + months(1:120),\n                      month = 1:120,\n                      pH = south.island.long$pH)\nSI.data$pH.tr <- SI.data$pH + c(rep(0, 60), (1:60) / 40)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(pH.tr ~ month, data = SI.data, col = \"red\")\npoints(pH ~ month, data = SI.data)\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-19-1.svg)\n:::\n:::\n\n\n## Linear regression\n\n### Original data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSI.lm <- lm(pH ~ month, data = SI.data)\nanova(SI.lm )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: pH\n           Df  Sum Sq  Mean Sq F value Pr(>F)\nmonth       1  0.0071 0.007149  0.0632 0.8019\nResiduals 118 13.3446 0.113090               \n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(SI.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = pH ~ month, data = SI.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.67415 -0.21847  0.03108  0.18078  1.03340 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  7.7050644  0.0617834 124.711   <2e-16 ***\nmonth       -0.0002228  0.0008862  -0.251    0.802    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3363 on 118 degrees of freedom\nMultiple R-squared:  0.0005355,\tAdjusted R-squared:  -0.007935 \nF-statistic: 0.06322 on 1 and 118 DF,  p-value: 0.8019\n```\n\n\n:::\n:::\n\n\n### Data with added slope\n\nWe first try a linear regression, and do plots for diagnosis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSI.tr.lm <- lm(pH.tr ~ month, data=SI.data)\nanova(SI.tr.lm )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: pH.tr\n           Df Sum Sq Mean Sq F value    Pr(>F)    \nmonth       1 22.259  22.259  137.37 < 2.2e-16 ***\nResiduals 118 19.121   0.162                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(SI.tr.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = pH.tr ~ month, data = SI.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.91378 -0.27110  0.05967  0.31867  0.93692 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 7.320611   0.073956   98.99   <2e-16 ***\nmonth       0.012433   0.001061   11.72   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4025 on 118 degrees of freedom\nMultiple R-squared:  0.5379,\tAdjusted R-squared:  0.534 \nF-statistic: 137.4 on 1 and 118 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(SI.tr.lm, which=1, pch=16)\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-22-1.svg)\n:::\n\n```{.r .cell-code}\nplot(pH.tr ~ month, data=SI.data, pch=16)\nlines(fitted(SI.tr.lm) ~ month,\n data=SI.data, type=\"l\", lwd=2, lty=2)\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-22-2.svg)\n:::\n:::\n\n\nThe coefficient for month is very significant, but we should check\nthe residuals to see if the fit is good or not.\n\n## Second degree polynomial\n\nWe now try a second degree polynomial.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSI.tr2.lm <- lm(pH.tr ~ month + I(month^2), data=SI.data)\nplot(SI.tr2.lm, which = 1, pch = 16)\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-23-1.svg)\n:::\n\n```{.r .cell-code}\nplot(pH.tr ~ month, data = SI.data, pch = 16)\nlines(fitted(SI.tr2.lm) ~ month,\n data=SI.data, type = \"l\", lwd = 2, lty = 2)\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-23-2.svg)\n:::\n\n```{.r .cell-code}\nsummary(SI.tr2.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = pH.tr ~ month + I(month^2), data = SI.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.71135 -0.25206  0.04066  0.17687  1.06588 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  7.778e+00  9.751e-02  79.765  < 2e-16 ***\nmonth       -1.006e-02  3.720e-03  -2.705  0.00786 ** \nI(month^2)   1.859e-04  2.979e-05   6.242 7.17e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3501 on 117 degrees of freedom\nMultiple R-squared:  0.6534,\tAdjusted R-squared:  0.6474 \nF-statistic: 110.3 on 2 and 117 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nBoth the linear and quadratic terms are significant.\n\nWe can also\ncompare the fits to the two models using `anova`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(SI.tr.lm, SI.tr2.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: pH.tr ~ month\nModel 2: pH.tr ~ month + I(month^2)\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1    118 19.121                                  \n2    117 14.345  1    4.7765 38.959 7.169e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n## Two separate linear regressions\n\nBecause we creted the data artifically we know that there is a \"change point\"\nhalf way into the time series. We can fit two separate linear models, one to\neacch half of the data, and compare them to that fitted to all data, shown above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSI.tr.lm <- lm(pH.tr ~ month, data=SI.data)\nSI.tr.A.lm <- lm(pH.tr ~ month, data=SI.data,\n subset=month<=60)\nSI.tr.B.lm <- lm(pH.tr ~ month, data=SI.data,\n subset=month>60)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(pH.tr ~ month, data=SI.data, pch=16)\nabline(SI.tr.lm, col=\"black\", lty=2, lwd=2)\nabline(SI.tr.A.lm, col=\"red\", lty=3, lwd=2)\nabline(SI.tr.B.lm, col=\"blue\", lty=4, lwd=2)\nlegend(2,9,\n legend=c(\"All\", \"month <= 24\", \"month > 24\"),\n col=c(\"black\",\"red\",\"blue\"), lty=c(2,3,4), lwd=2)\ntitle(\"Linear regressions\")\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-26-1.svg)\n:::\n:::\n\n\n### The three fitted regression models\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(SI.tr.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: pH.tr\n           Df Sum Sq Mean Sq F value    Pr(>F)    \nmonth       1 22.259  22.259  137.37 < 2.2e-16 ***\nResiduals 118 19.121   0.162                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(SI.tr.A.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: pH.tr\n          Df Sum Sq Mean Sq F value Pr(>F)\nmonth      1 0.0009 0.00088  0.0065 0.9362\nResiduals 58 7.8947 0.13612               \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(SI.tr.B.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: pH.tr\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \nmonth      1 11.3569 11.3569  120.92 8.173e-16 ***\nResiduals 58  5.4474  0.0939                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(SI.tr.B.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = pH.tr ~ month, data = SI.data, subset = month > \n    60)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6426 -0.1805  0.0650  0.1597  0.6082 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 6.172128   0.210506   29.32  < 2e-16 ***\nmonth       0.025122   0.002285   11.00 8.17e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3065 on 58 degrees of freedom\nMultiple R-squared:  0.6758,\tAdjusted R-squared:  0.6702 \nF-statistic: 120.9 on 1 and 58 DF,  p-value: 8.173e-16\n```\n\n\n:::\n:::\n\n\nIn the separate fits, the estimate of the coefficient for month (slope) is now\nclose to that used to generate the data.\n\n# Chow's _F_ test\n\nWith real data, we would not know the position or even whether there is\nsignificanr a change point in the data. We can use Chow's _F_ test to test if\nfitting two separate regressions according to a known change point, as just\nshown, is significantly better than fitting a single regression.\n\nThe models are fitted by ordinary least squares (OLS).\nAn $F$-value is calculated based on the residuals. The numerator\nmeasures how much the fit improves (how much the residuals\ndecrease) by fitting two separate lines. The denominator measures\nthe error based on the residuals for the `bigger' model (two\nseparate regressions).\n\nThe test proposed by Chow for a known change point can be extended\nto the case of an unknown change point.\n\nThese tests are used to find a single change point in the series.\nThey do not test for whether there is a trend or not, they test\nfor a change in the 'structure' of the series.\n\nAs implemented they can be used for testing an unreplicated\nseries, so we use the mean of the five observations from each\nmonth's sample.\n\nAs we just saw the procedure is to divide the data into two parts,\nbefore and after the possible change point and to do two separate\nfits, one for each part of the data, and then compare these fits\nto fitting a single curve to all data.\n\nThe difference is that now we test all possible change points in a\ngiven interval.\n\nConsequently, the _F_-statistic is calculated for all possible\nchange points in an interval.\n\nThese _F_-statistic values can be plotted or used to calculate the\nprobability for the existence of a change point.\n\nFirst we look for a change in slope or intercept, in the original\ndata set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(strucchange)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: zoo\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'zoo'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: sandwich\n```\n\n\n:::\n\n```{.r .cell-code}\nSI.fs <- Fstats(pH ~ month, from = 0.25,\n data=SI.data)\nsctest(SI.fs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tsupF test\n\ndata:  SI.fs\nsup.F = 5.6912, p-value = 0.3482\n```\n\n\n:::\n:::\n\n\nThen we look for a change in slope or intercept, in the data set\nwith a trend.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSI.tr.fs <- Fstats(pH.tr ~ month, from = 0.25,\n data=SI.data)\n\n sctest(SI.tr.fs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tsupF test\n\ndata:  SI.tr.fs\nsup.F = 57.805, p-value = 1.161e-11\n```\n\n\n:::\n:::\n\n\n# Generalized fluctuation tests\n\nThese are based either on estimates of regression coefficients or on residuals.\nEstimates from fits to all data are compared to those from subsamples of the\ndata. One way of choosing the subsamples is using a moving window. If there are\nno structural changes the estimates from all data and from subsamples should be\nsimilar.\n\nIf there are structural changes, the true coefficients will change over time,\nand the estimates from some subsamples of the data will differ... or the\nresiduals will differ.\n\nWe again first test the original data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSI.efp <- efp(pH ~ month, type=\"OLS-MOSUM\",\n data=SI.data)\nplot(SI.efp)\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-33-1.svg)\n:::\n\n```{.r .cell-code}\nsctest(SI.efp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOLS-based MOSUM test\n\ndata:  SI.efp\nM0 = 0.90808, p-value = 0.271\n```\n\n\n:::\n:::\n\n\nAnd then we test the data with a trend added.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSI.tr.efp <- efp(pH.tr ~ month, type=\"OLS-MOSUM\",\n data=SI.data)\nplot(SI.tr.efp)\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-34-1.svg)\n:::\n\n```{.r .cell-code}\nsctest(SI.tr.efp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOLS-based MOSUM test\n\ndata:  SI.tr.efp\nM0 = 1.8009, p-value = 0.01\n```\n\n\n:::\n:::\n\n\nThe empirical fluctuation process, in the this case the sum over a\nmoving window of the residuals, crosses the $\\alpha = 0.05$\nboundary line at about the place were the change in slope starts.\n\n# Fitting a model with a hinge\n\nLinear models allow a lot of flexibility. It is important to understand that\nmodels representing the same fitted lines or curves can be fitted using\ndifferent model formulas, which result in different tests of significance. We\nwill go in this section through multiple examples. In all cases, one should\nplot the predictions from the fitted model together with the data, as this is\na very effective way of judging the suitability of the model. This was\ndemonstrated by Anscombe's artificial data, but applies to any data set and\nmodel.\n\n## Using a factor\n\nWhen there is a knwon change point in the data (as we detected with the\ntests above), we need to fit a model that corresponds to a function that\ncan have a knot or hinge. The simplest such model is two straight lines joined\nat a knot. This is the simplest possible case of what is called a _linear spline_.\n\nThe first stept is to create a factor that informs whether observations apear\nbefore or after the change point.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSI.data$half <- factor(ifelse(SI.data$month < 60, \"first\", \"second\"))\n```\n:::\n\n\nIn the model fitted below `month` is a numeric variable and `half` a factor\nwith two levels.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmflspl1 <- lm(pH.tr ~ month * half, data = SI.data)\nsummary(mflspl1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = pH.tr ~ month * half, data = SI.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.67407 -0.21951  0.03008  0.17979  1.03279 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       7.7080479  0.0894434  86.178  < 2e-16 ***\nmonth            -0.0002852  0.0025928  -0.110    0.913    \nhalfsecond       -1.5278734  0.2432220  -6.282 6.04e-09 ***\nmonth:halfsecond  0.0253270  0.0035785   7.078 1.20e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3392 on 116 degrees of freedom\nMultiple R-squared:  0.6776,\tAdjusted R-squared:  0.6692 \nF-statistic: 81.25 on 3 and 116 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n## Using dummy variables\n\nInstead of using a factor we can use dummy variables, numeric variables that take either\n0 or 1 as value. With start with the equivalent of the contrasts that R used by default\nin the example immediately above. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nSI.data$dummy.early <- ifelse(SI.data$month < 60, 1, 0)\nSI.data$dummy.late <- ifelse(SI.data$month < 60, 0, 1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmflspl2 <- lm(pH.tr ~  1 + month + I(1 * dummy.late) + I(month * dummy.late), data = SI.data)\nsummary(mflspl2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = pH.tr ~ 1 + month + I(1 * dummy.late) + I(month * \n    dummy.late), data = SI.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.67407 -0.21951  0.03008  0.17979  1.03279 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            7.7080479  0.0894434  86.178  < 2e-16 ***\nmonth                 -0.0002852  0.0025928  -0.110    0.913    \nI(1 * dummy.late)     -1.5278734  0.2432220  -6.282 6.04e-09 ***\nI(month * dummy.late)  0.0253270  0.0035785   7.078 1.20e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3392 on 116 degrees of freedom\nMultiple R-squared:  0.6776,\tAdjusted R-squared:  0.6692 \nF-statistic: 81.25 on 3 and 116 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nWe now will set up the model differently. You will need to work out what is\nthe difference with respect to the estimates and their significance. Note\nthat in this case \"- 1\" is needed to remove the overall intercept from the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmflspl3 <- lm(pH.tr ~  I(1 * dummy.early) + I(month * dummy.early) +\n                I(1 * dummy.late) + I(month * dummy.late) - 1, data = SI.data)\nsummary(mflspl3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = pH.tr ~ I(1 * dummy.early) + I(month * dummy.early) + \n    I(1 * dummy.late) + I(month * dummy.late) - 1, data = SI.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.67407 -0.21951  0.03008  0.17979  1.03279 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \nI(1 * dummy.early)      7.7080479  0.0894434   86.18   <2e-16 ***\nI(month * dummy.early) -0.0002852  0.0025928   -0.11    0.913    \nI(1 * dummy.late)       6.1801745  0.2261788   27.32   <2e-16 ***\nI(month * dummy.late)   0.0250418  0.0024663   10.15   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3392 on 116 degrees of freedom\nMultiple R-squared:  0.9983,\tAdjusted R-squared:  0.9982 \nF-statistic: 1.706e+04 on 4 and 116 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n### Plotting the fitted values\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSI.data$pH.tr.fitted1 <- fitted(mflspl1)\nSI.data$pH.tr.fitted2 <- fitted(mflspl2)\nSI.data$pH.tr.fitted3 <- fitted(mflspl3)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(pH.tr ~ month, data = SI.data)\nlines(pH.tr.fitted1 ~ month, data = SI.data)\nlines(pH.tr.fitted2 ~ month, data = SI.data, col = \"red\")\nlines(pH.tr.fitted3 ~ month, data = SI.data, col = \"Blue\")\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-41-1.svg)\n:::\n:::\n\n### Plotting the predicted values\n\nAs R users nowadays use package 'ggplot2' instead of R's own functions for \nplotting. In the rest of this archive I use R's `plot()` but in the next\ntwo examples I use `ggplot()` as in this case is simpler to use.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew.data <- data.frame(month = 1:120)\nnew.data$half <- factor(ifelse(new.data$month < 60, \"first\", \"second\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\npredict.data <- cbind(new.data, \n                      predict(mflspl1, new.data, interval = \"confidence\")) \nggplot(data = predict.data) +\n  geom_point(data = SI.data, \n             mapping = aes(x = month, y = pH.tr)) +\n  geom_ribbon(alpha = 0.25, color = NA, \n              mapping = aes(x = month, y = fit, ymax = upr, ymin = lwr)) +\n  geom_line(color = \"red\", mapping = aes(x = month, y = fit))\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-43-1.svg)\n:::\n:::\n\n\nFor a predition with the third model we need new data containing the dummy\nvariables, instead of the factor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew.data <- data.frame(month = 1:120)\nnew.data$dummy.early <- ifelse(new.data$month < 60, 1, 0)\nnew.data$dummy.late <- ifelse(new.data$month < 60, 0, 1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\npredict.data <- cbind(new.data, predict(mflspl3, new.data, interval = \"confidence\")) \nggplot(data = predict.data) +\n  geom_point(data = SI.data, mapping = aes(x = month, y = pH.tr)) +\n  geom_ribbon(alpha = 0.25, color = NA, mapping = aes(x = month, y = fit, ymax = upr, ymin = lwr)) +\n  geom_line(color = \"red\", mapping = aes(x = month, y = fit))\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-45-1.svg)\n:::\n:::\n\n\nWe can see that even though the parametrization is different the fit and its\nconfidence band remain the same.\n\n### Questions\n\n**A)** What is the difference between the two approaches?\n**B)** What do the parameter estimates represent in each case?\n**C)** Although you get different estimates from the two models, can you calculate the same value for the slope in both cases?\n\n# Robust and resistant statistics\n\nIf changing a small part of the body of data, perhaps drastically,\ncan change the value of the summary substantially, the summary is\nnot resistant. Example: the **mean**.\n\nConversely, if a change of a small part of the data, no matter\nwhat part or how substantially, fails to change the summary\nsubstantially, the summary is said to be _resistant_.\nExample: the **median**.\n\n_Robustness:_ lack of susceptibility to the effects of\nnon-normality.\n\n_Robustness of validity._ example: confidence interval for\nthe population median based on the sign test.\n\n_Robustness of efficiency:_ how much information is\nextracted from the data under different situations.\n\nA robust statistic performs well under different circumstances.\n\n# Robust and resistant regression\n\nRobust regression methods are at least partially resistant to\noutliers, those called resistant methods are even more resistant.\n\nThere are several methods that can be used for robust regression.\n\nThere are also several methods for resistant regression.\n\nIn most cases they yield only approximate solutions calculated by\niteration.\n\nWe will not discuss the complexities of how they work, but give an\nexample of the use of weighted likelihood, a recently developed\nmethod.\n\nAs an oversimplification it can be said that this method assigns a\nweight to each observation based on the relationship between a\nchosen model distribution and the empirical cumulative\ndistribution.\n\n## Data with outliers\n\nWe first add six outliers to the pH data from rivers of the South\nIsland of New Zealand. That is to the original data rather than those data with\nthe added change point we fitted models to in the examples immediatley above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nattach(SI.data)\nSI.data$pH.out <- SI.data$pH\nSI.data$pH.out[110:115] <- SI.data$pH[110:115] + 2.0\ndetach()\n```\n:::\n\n\n\n## Linear regression by OLS\n\n**OLS** = ordinary least squares, the most usual method for model fitting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSI.out.lm <- lm(pH.out ~ month, data=SI.data)\nsummary(SI.out.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = pH.out ~ month, data = SI.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.92618 -0.32155 -0.07075  0.20746  1.79671 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 7.542880   0.095734  78.790  < 2e-16 ***\nmonth       0.004111   0.001373   2.994  0.00336 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5211 on 118 degrees of freedom\nMultiple R-squared:  0.07058,\tAdjusted R-squared:  0.06271 \nF-statistic: 8.961 on 1 and 118 DF,  p-value: 0.00336\n```\n\n\n:::\n:::\n\n\nBecause of the six outliers, the slope for month is significantly\ndifferent from zero.\n\n### Plotting the prediction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew.data <- data.frame(month = 1:120)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\npredict.out.data <- cbind(new.data, \n                      predict(SI.out.lm, new.data, interval = \"confidence\")) \nggplot(data = predict.out.data) +\n  geom_point(data = SI.data, \n             mapping = aes(x = month, y = pH.out)) +\n  geom_ribbon(alpha = 0.25, color = NA, \n              mapping = aes(x = month, y = fit, ymax = upr, ymin = lwr)) +\n  geom_line(color = \"red\", mapping = aes(x = month, y = fit))\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-49-1.svg)\n:::\n:::\n\n\n## Robust regression with wle (weighted likelihood)\n\nWLE works by assuming a given distribution for the residuals, the Normal. Then\nuses the likelihood of the residuals and downweights those in the empirical\ndistribution that are exceptional compared to the Normal.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nSI.out.rlm <- rlm(pH.out ~ month, data=SI.data)\nsummary(SI.out.rlm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall: rlm(formula = pH.out ~ month, data = SI.data)\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.724764 -0.233627  0.005976  0.234966  1.995454 \n\nCoefficients:\n            Value    Std. Error t value \n(Intercept)   7.6332   0.0710   107.5462\nmonth         0.0015   0.0010     1.5029\n\nResidual standard error: 0.3489 on 118 degrees of freedom\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(SI.out.rlm, ask = FALSE)\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-50-1.svg)\n:::\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-50-2.svg)\n:::\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-50-3.svg)\n:::\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-50-4.svg)\n:::\n:::\n\n\nNow the outliers are ignored, and  the coefficient for\nmonth does not differ significantly from zero.\n\n## Plotting the lines\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(pH.out ~ month, data=SI.data, pch=16)\nlines(fitted(SI.out.rlm) ~ month, data=SI.data)\nlines(fitted(SI.out.lm) ~ month, data=SI.data, lty=2)\nlegend(x = 0, y = 9.5, legend=c(\"lm\", \"rlm\"), lty=c(2,1))\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-51-1.svg)\n:::\n:::\n\n\n# $x$ measured without error\n\nLet's see what happens if we assume that `pH.out` is measured without error, by\nsymply swapping the places of `month` and `pH.out` in the model formula.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSI.out.xy.lm <- lm(month ~ pH.out, data=SI.data)\nsummary(SI.out.xy.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = month ~ pH.out, data = SI.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-62.765 -28.513   2.279  25.395  71.203 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  -73.281     44.796  -1.636  0.10452   \npH.out        17.170      5.736   2.994  0.00336 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 33.68 on 118 degrees of freedom\nMultiple R-squared:  0.07058,\tAdjusted R-squared:  0.06271 \nF-statistic: 8.961 on 1 and 118 DF,  p-value: 0.00336\n```\n\n\n:::\n:::\n\n### Question\n\n  1. How different do you expect the lines to be?\n\n### Plotting the prediction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew.data <- data.frame(pH.out = seq(from = 7, to = 10, length.out = 120))\npredict.xy.data <- cbind(new.data, \n                         predict(SI.out.xy.lm, new.data, interval = \"confidence\")) \n```\n:::\n\n\nWe first plot according to the formulation of the model we have fitted.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = predict.xy.data) +\n  geom_point(data = SI.data, \n             mapping = aes(y = month, x = pH.out)) +\n  geom_ribbon(alpha = 0.25, color = NA, \n              mapping = aes(y = fit, x = pH.out, ymax = upr, ymin = lwr)) +\n  geom_line(color = \"red\", mapping = aes(y = fit, x = pH.out))\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-54-1.svg)\n:::\n:::\n\n\nThe line in the plot above looks as expected. Even though we have fitted a \nlinear regression of `month` on `pH.out` we will\nstill flip the _x_- and _y_-axes so that it is easier to compare to \nearlier figures.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = predict.xy.data) +\n  geom_point(data = SI.data, \n             mapping = aes(y = month, x = pH.out)) +\n  geom_ribbon(alpha = 0.25, color = NA, \n              mapping = aes(y = fit, x = pH.out, ymax = upr, ymin = lwr)) +\n  geom_line(color = \"red\", mapping = aes(y = fit, x = pH.out)) +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-55-1.svg)\n:::\n:::\n\n\n## Major axis regression\n\nThe example above only demonstrates how different the fitted lines can be, but\nin practice, given the extreme outliers, it does not make sense to assume that\nthey are not exceptional. This does not necesarily mean that we can ignore or\nremove these observations, as they may contain important information, but\npossibly they will need to be studied as separate cases.\n\nWe will use package 'lmodel2' and a different example data set that is a real\nuse case. If you have data from _surveys_ where you want to study relationships\nbetween measured variables that are subject to random variation then OLS linear\nregression is likely to bias the slope estimates. For details read the _vignette_\nthat accompanies 'lmodel2'.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lmodel2)\n```\n:::\n\n\nAn example taken from the documentation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Example 1 (surgical unit data)\ndata(mod2ex1)\nEx1.res <- lmodel2(Predicted_by_model ~ Survival, data=mod2ex1, nperm=99)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRMA was not requested: it will not be computed.\n```\n\n\n:::\n\n```{.r .cell-code}\nEx1.res\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nModel II regression\n\nCall: lmodel2(formula = Predicted_by_model ~ Survival, data = mod2ex1,\nnperm = 99)\n\nn = 54   r = 0.8387315   r-square = 0.7034705 \nParametric P-values:   2-tailed = 2.447169e-15    1-tailed = 1.223585e-15 \nAngle between the two OLS regression lines = 9.741174 degrees\n\nPermutation tests of OLS, MA, RMA slopes: 1-tailed, tail corresponding to sign\nA permutation test of r is equivalent to a permutation test of the OLS slope\nP-perm for SMA = NA because the SMA slope cannot be tested\n\nRegression results\n  Method Intercept     Slope Angle (degrees) P-perm (1-tailed)\n1    OLS 0.6852956 0.6576961        33.33276              0.01\n2     MA 0.4871990 0.7492103        36.84093              0.01\n3    SMA 0.4115541 0.7841557        38.10197                NA\n\nConfidence intervals\n  Method 2.5%-Intercept 97.5%-Intercept 2.5%-Slope 97.5%-Slope\n1    OLS      0.4256885       0.9449028  0.5388717   0.7765204\n2     MA      0.1725753       0.7633080  0.6216569   0.8945561\n3    SMA      0.1349629       0.6493905  0.6742831   0.9119318\n\nEigenvalues: 0.1332385 0.01090251 \n\nH statistic used for computing C.I. of MA: 0.007515993 \n```\n\n\n:::\n\n```{.r .cell-code}\nplot(Ex1.res) \n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-57-1.svg)\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## Example 2 (eagle rays and Macomona)\ndata(mod2ex2)\nEx2.res <- lmodel2(Prey ~ Predators, data=mod2ex2, \"relative\", \"relative\", 99)\nEx2.res\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nModel II regression\n\nCall: lmodel2(formula = Prey ~ Predators, data = mod2ex2, range.y =\n\"relative\", range.x = \"relative\", nperm = 99)\n\nn = 20   r = 0.8600787   r-square = 0.7397354 \nParametric P-values:   2-tailed = 1.161748e-06    1-tailed = 5.808741e-07 \nAngle between the two OLS regression lines = 5.106227 degrees\n\nPermutation tests of OLS, MA, RMA slopes: 1-tailed, tail corresponding to sign\nA permutation test of r is equivalent to a permutation test of the OLS slope\nP-perm for SMA = NA because the SMA slope cannot be tested\n\nRegression results\n  Method Intercept    Slope Angle (degrees) P-perm (1-tailed)\n1    OLS  20.02675 2.631527        69.19283              0.01\n2     MA  13.05968 3.465907        73.90584              0.01\n3    SMA  16.45205 3.059635        71.90073                NA\n4    RMA  17.25651 2.963292        71.35239              0.01\n\nConfidence intervals\n  Method 2.5%-Intercept 97.5%-Intercept 2.5%-Slope 97.5%-Slope\n1    OLS      12.490993        27.56251   1.858578    3.404476\n2     MA       1.347422        19.76310   2.663101    4.868572\n3    SMA       9.195287        22.10353   2.382810    3.928708\n4    RMA       8.962997        23.84493   2.174260    3.956527\n\nEigenvalues: 269.8212 6.418234 \n\nH statistic used for computing C.I. of MA: 0.006120651 \n```\n\n\n:::\n\n```{.r .cell-code}\nop <- par(mfrow = c(1,2))\nplot(Ex2.res, \"OLS\")\nplot(Ex2.res, \"RMA\")\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-58-1.svg)\n:::\n\n```{.r .cell-code}\npar(op)\n```\n:::\n",
    "supporting": [
      "linear-models-intro_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}