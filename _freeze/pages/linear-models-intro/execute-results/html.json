{
  "hash": "a6f7f71733da7a19b19949f581bce58f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear Models\"\nsubtitle: \"An introduction using `lm()`\"\nauthor: \"Pedro J. Aphalo\"\ndate: 2023-11-27\ndate-modified: 2023-11-30\ncategories: [R, model fitting]\nkeywords: [predicted values, residual values, parameter estimates, model formulas]\nformat:\n  html:\n    mermaid:\n      theme: neutral\ncode-fold: true\ncode-tools: true\ncallout-icon: false\nengine: knitr\nfilters:\n  - webr\nabstract: \n  In this page I give a brief presentation on the mechanics of fitting statistical models to observed data using R. I use a lineal model (LM), polynomial regression, as example. I describe the most important values contained in the model fit object returned by function `lm()` and how to extract them. I rely heavily on diagrams and data plots.\n---\n\n\n# Introduction\n\n_Fitting a model_ to data consists in finding the parameter values that best\nexplain the data or observations. These parameter values are estimates of\nthose in a larger population of possible observations from which we have\ndrawn a sample or subset. For deciding what is best, we need a criterion, and\nthe most frequently used one is minimizing the \"residual variance\" by the\nmethod of ordinary least squares (OLS).\n\n_Model selection_ involves comparing models that differ in their structure, i.e., \nin the formula that relates parameters and data. Here we also need a criterion\nto \"measure\" how well a model describes the data, for example, AIC or BIC,\nwhich are information criteria. Alternatively, we can just choose \"the simplest\nmodel that does a good enough job\".\n\nDifferent approaches to model fitting make different assumptions about the \nobservations. What assumptions are reasonable for our data determines what \nmethods are applicable. Out of these, we can choose the one that best suits\nthe nature of our scientific or practical question or hypothesis.\n\nModern statistical algorithms, and their implementation in R, strive to be\nwidely applicable. I will emphasize this as well as what is common to different\nmethods.\n\nIn general, there are many different aspects of a model fit that we may be\ninterested in. The most computationally intensive step is the fitting itself.\nThus R's approach is to save the results from fitting, or _fitted model_, and\nseparately query it for different derived numerical and graphical output as \nneeded.\n\n::: callout-tip\nIn this page some code chunks are \"folded\" so as to decrease the clutter. Above the R\noutput, text or plots, you will find a small triangle followed by \"Code\".\nClicking on the triangle \"unfolds\" the code chunk making visible the R code used\nto produce the output shown.\nThe code in the chunks can be copied by clicking on the top right corner, where\nan icon appears when the mouse cursor hovers over the code listing.\n:::\n\nAttach packages used for plotting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggpmisc)\nlibrary(ggbeeswarm)\nlibrary(broom)\ntheme_set(theme_bw(16))\n```\n:::\n\n\n::: callout-caution\n# Anscombe's linear regression examples\n\nThis classical example from Anscombe (1973) demonstrates four very different data sets that yield exactly the same results when a linear regression model is fit to them, including $R^2$ and $P$ values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# we rearrange the data\nmy.mat <- matrix(as.matrix(anscombe), ncol=2)\nmy.anscombe <- \n  data.frame(x = my.mat[ , 1],\n             y = my.mat[ , 2],\n             case=factor(rep(1:4, rep(11,4))))\nmy.formula = y ~ x\nggplot(my.anscombe, aes(x,y)) +\n  geom_point(shape=21, fill=\"orange\", size=3) +\n  geom_smooth(method=\"lm\", formula = my.formula) +\n  stat_poly_eq(formula = my.formula, \n               parse = TRUE, \n               label.y = \"top\", \n               label.x = \"left\", \n               use_label(c(\"eq\", \"R2\", \"P\"))) +\n  facet_wrap(~case, ncol=2) +\n  theme_bw(16)\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-2-1.svg)\n:::\n:::\n\n  1.  If these four sets of data were real observations from an experiment, would you conclude the same in all cases with respect to the effect of $x$ on the observed $y$?\n  1.  In which of the four cases the estimated line best described the _overall trend_ of the response?\n  1-  What is the difficulty in each of the other three cases?\n\nThese concocted data examples demonstrates that when fitting any model to data, extreme caution is always needed. **We need to check graphically the observations and how well model estimates fit them.** $R^2$, $P$ values, and other parameter's estimates _must always be considered in the light of the data, rather than in isolation_.\n:::\n\n# Model-fitting core ideas\n\n\n```{mermaid}\nflowchart TB\ns[Total variation] --> f(Model fitting)-->|origin explained| m[Model parameters]\nf -->|origin not explained| e[Experimental error]\n```\n\n\nWhat is left as experimental error is the \"residual\" variation, what the model we fitted could not explain (= describe). _The error estimate is the result of the interaction between data and model._\n\nThe next idea is that the variance that is explained by a model can be \"partitioned\" so that different \"sources\" of variation can be compared and their importance assessed.\n\n\n```{mermaid}\nflowchart TB\ns[Total variation] --> f(Model fitting)-->|origin explained| m[Model parameters]\nf --->|origin not explained| e[Experimental error]\nm --> s1[y variation due to x1]\nm --> s2[y variation due to x2]\nm --> s3[y variation due to x3]\n```\n\n\n::: callout-tip\n# Differences among fitted models\n\n- In many cases we can choose among different models to fit.\n\n- Sometimes, models differ only in how we partition the same explained variation, facilitating different interpretations of the data. \n\n- In many other cases different models differ in how much of the total variation they can explain and how much remains unexplained.\n:::\n\n::: callout-tip\n# Model formulas\n\nIn statistics models are described by equations, and in R using _model formulas_, which are similar to these equations.\n\n_A _model formula_ defines the nature/$\\approx$shape of the relationship between dependent ($y$) and independent or explanatory variables ($x_i$)._\n\nWhen a model is fitted numeric values to insert into the model formula are searched for. The selected values are those that explain as much of the variation among observations as possible, within the constraint of the given model formula.\n\nHow the estimates are computed depends on the type of model being fitted to data and the algorithm used. However, the objective of the computations is always the same, minimizing the residual variation. (It is not necessary to always to measure the variability as variance or to use ordinary least squares, OLS, methods. OLS is, however, most effective _as long as errors are normally distributed_.)\n:::\n\n# Linear models (LM)\n\nI will start with some \"unusual examples\" (crazy approaches?) to try to convince you that there is much in common between model fitting and the simple statistics you already know. Grasping this is important as general concepts allows us to understand the bigger picture, and thus we can learn new methods as variations of, or extensions to, what we already know.\n\n## The mean (= average) as a fitted linear model\n\nWe compute the mean of sample as ${\\bar x} = \\frac{\\sum_{i=1}^n x_i}{n}$ as an estimator of $\\mu$, the mean of the population from which the sample was drawn.\n\nAn example with R follows.\n\nWe create a vector of 20 random numbers from the Normal distribution to play with.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmy.vector <- rnorm(20)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmean(my.vector)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.1691184\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nvar(my.vector)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5530737\n```\n\n\n:::\n:::\n\n**We may ask:** _Can we also obtain these estimators by fitting a lineal model?_\n\nThe answer is, \"Yes! we can.\"\n\nWe assume that the observations $y_{ij}$ follow the linear model\n$$\ny_{ij} = \\mu + \\epsilon_{ij},\n$$\nand fit model to the data to obtain an estimate of $\\mu$.\n\nThis model in R, given that we have given the name `my.vector` to the observation, is represented by the formula `my.vector ~ 1`. This is a linear model, with only an intercept as only parameter. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nfm1 <- lm(my.vector ~ 1)\n```\n:::\n\nCalling `summary()` on the result from fitting the model we get an estimate for the intercept, which is the same $\\bar x$ estimate we computed above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fm1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = my.vector ~ 1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.36760 -0.45614 -0.04398  0.48916  1.36646 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  -0.1691     0.1663  -1.017    0.322\n\nResidual standard error: 0.7437 on 19 degrees of freedom\n```\n\n\n:::\n:::\n\n\nThe ANOVA (analysis of variance) table, unsurprisingly, shows variance under the name of _mean square_ (MS). The residual mean square, or error variance, or residual variance gives the same value we calculated above using function `var()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(fm1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: my.vector\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals 19 10.508 0.55307               \n```\n\n\n:::\n:::\n\n\nTo fit the mean as a linear model, the total \"Sum Sq\", or SS, was minimized.\n\nSo fitting a model is in fact similar to computing the mean of a single variable. However, while the mean is a single parameter, we can fit models with several parameters which are adjusted simultaneously to fit a point, curve, surface or hyper-surface, depending on the case.\n\nWhen fitting a model we normally also estimate the standard deviation for each fitted parameter, allowing tests of significance.\n\n### The _t_-test as a fitted model linear model\n\nTwo vectors of numbers to play with.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nvec1 <- 0 + rnorm(20)\nvec2 <- 1 + rnorm(20)\n```\n:::\n\n\nWe can assemble a `data.frame` with them, adding a factor with two levels representing groups.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ndf1 <- data.frame(obs = c(vec1, vec2), \n                  group = factor(rep(c(\"g1\", \"g2\"), each = 20)))\nhead(df1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          obs group\n1  0.04613973    g1\n2  0.52830879    g1\n3 -0.58804178    g1\n4 -0.03156064    g1\n5 -0.52437692    g1\n6  0.09804958    g1\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nsummary(df1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      obs          group  \n Min.   :-1.2520   g1:20  \n 1st Qu.:-0.3278   g2:20  \n Median : 0.1187          \n Mean   : 0.4675          \n 3rd Qu.: 1.1726          \n Max.   : 2.3403          \n```\n\n\n:::\n:::\n\nWe can plot the data adding means and standard errors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df1, aes(group, obs)) +\n  stat_summary(fun.data = mean_se, color = \"red\") +\n  geom_beeswarm()\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-12-1.svg)\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmean(vec1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.2129308\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nmean(vec2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.147908\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nvar(vec1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2716516\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nvar(vec2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8382209\n```\n\n\n:::\n:::\n\nOne way of calling `t.test()` is by passing the data separately for each group. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nt.test(vec1, vec2, var.equal = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  vec1 and vec2\nt = -5.7768, df = 38, p-value = 1.152e-06\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.8377274 -0.8839509\nsample estimates:\n mean of x  mean of y \n-0.2129308  1.1479083 \n```\n\n\n:::\n:::\n\n\nWe can do the same test calling `t.test()` using _model formula_ syntax. The model formula `obs ~ group`, used here, corresponds to the model formulated as $y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}$.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nt.test(obs ~ group, data = df1, var.equal = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  obs by group\nt = -5.7768, df = 38, p-value = 1.152e-06\nalternative hypothesis: true difference in means between group g1 and group g2 is not equal to 0\n95 percent confidence interval:\n -1.8377274 -0.8839509\nsample estimates:\nmean in group g1 mean in group g2 \n      -0.2129308        1.1479083 \n```\n\n\n:::\n:::\n\n\n**We may ask:** _Can we test significance like with the t-test by fitting a lineal model?_\n\nThe answer is, \"Yes! we can.\"\n\n::: callout-caution\n# Intercept in R model formulas\n\nModel formulas `obs ~ 1 + group` and `obs ~ group` are equivalent as the intercept is included by default. The second form is the most commonly used. Here, to make the comparison to the model equations easier, I include the `1 +` term explicitly.\n:::\n\nThe main difference in the call to `lm()` is the missing `var.equal = TRUE`, as equal variances are always assumed by `lm()`.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nfm2 <- lm(obs ~ 1 + group, data = df1)\nsummary(fm2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = obs ~ 1 + group, data = df1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.95549 -0.35161  0.04613  0.59894  1.19240 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.2129     0.1666  -1.278    0.209    \ngroupg2       1.3608     0.2356   5.777 1.15e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7449 on 38 degrees of freedom\nMultiple R-squared:  0.4676,\tAdjusted R-squared:  0.4536 \nF-statistic: 33.37 on 1 and 38 DF,  p-value: 1.152e-06\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nanova(fm2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: obs\n          Df Sum Sq Mean Sq F value    Pr(>F)    \ngroup      1 18.519 18.5188  33.371 1.152e-06 ***\nResiduals 38 21.088  0.5549                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nWith the three approaches, the _P_-value obtained is the same. The estimate of the difference the means of `g1` and `g2` is given in the summary for the model fit with `lm()` while `t.test` shows the two means. The square of the estimated _t_-value matches the _F_-value in the ANOVA table.\n\nSo, these examples show that model fitting provides estimates of population parameters, identically to computing a mean, as well as serving as the basis for tests of significance identical to the _t_-test. In addition, as we will see below, linear models include additional models and  tests of significance to those possible with _t_-test.\n\n::: callout-note\nThe _implementation_ in R of _t_-tests by default does not assume equal variances in the two groups that are compared. Linear models, as implemented in `lm()` do.\n\nOther R model-fitting functions cater for various additional cases.\n:::\n\n### ANOVA, regression and linear models\n\nLinear models include what is called Analysis of Variance (ANOVA), linear regression, multiple regression, Analysis of Covariance, and a few other variations all wrapped together. So, one procedure caters for a wide range of situations and designs of experiments. While the _t_-test can be used to test differences between means of two groups, or between a mean and a fixed value, ANOVA can be used with two or more groups.\n\nLinear models can be computed very efficiently but make several assumptions about the data. Other types of models, avoid some of the assumptions of LMs, but tend to be computationally more demanding.\n\n## ANOVA vs. Regression\n\nIn _one-way ANOVA_ for a completely randomized design (with no blocks) we have the\nfollowing model: every observation receives a contribution from the population\nmean contains $\\mu$, observations for the $i$th treatment receive a contribution\nfrom $\\tau_i$ but no other $\\tau$. Each observation in addition is affected by\nits own bit of \"random\" variation.\n\nThus, the observations $y_{ij}$ follow the linear model\n$$\ny_{ij} = \\mu + \\tau_i + \\epsilon_{ij},\n$$\nwhere $\\mu$ is the population mean, $\\tau_i$ the effect of\ntreatment $i$, and $\\epsilon_{ij}$ the random variation associated\nwith observation $y_{ij}$.\n\nWe assume that the residuals $\\epsilon_{ij}$\nare independent and equally distributed --- $\\epsilon_{ij} \\sim\nN(0,\\sigma^2)$, where variance $\\sigma^2$ is unknown and\nindependent of group $i$.\\vspace{2cm}\n\n---\n\nIn linear regression the $y$ values are obtained from several\npopulations, each population being determined by the corresponding\n$x$ value. The $y$ variable is called _dependent variable_\nand the $x$ variable is called _independent variable_.\n\nThe observations $y_{ij}$ follow the linear model\n$$\ny_{ij} = \\beta_0 + \\beta_1 x_i + \\epsilon_{ij},\n$$\n\nwhere $\\beta_0$ is the population mean when $x=0$, $x_i$ are the\nobserved values for the independent variable, $\\beta_1$ is the\ncoefficient describing the slope, and $\\epsilon_{ij}$ the random\nvariation associated with observation $y_{ij}$.\n\nWe also assume that the residuals\n$\\epsilon_{ij}$ are independent and equally distributed ---\n$\\epsilon_{ij} \\sim N(0,\\sigma^2)$, where variance $\\sigma^2$ is\nunknown and independent of $x$.\n\nIn regression estimates of all $\\beta_i$ are usually of interest. In contrast, in ANOVA, the estimate of $\\mu$ across all treatments is considered not interesting, and the focus of the analysis is on the estimate of $\\tau$.\n\n::: callout-tip\n# Assumption of Normality\n\n_What are the reasons why we so frequently assume that error variation follows a Normal distribution?_\n\n1. **Experience** The variation in the real world _rather frequently_ follows distributions that are rather similar to the Normal. _Many exceptions exist, too._\n\n1. **Convenience** Assuming a Normal distribution makes computations a lot easier than assuming other distributions. _Crucial in the absence of computers._ \n\n1. **Theory** The distribution of parameter estimates such as the mean tend to be distributed \"more normally\" than the observations from which they are computed. May explain, at least in part, the first point above.\n:::\n\n::: callout-tip\n# Why \"linear\" in the name\n\nLinear models receive the name \"linear\" because of how the parameters that are estimated enter the _model formula_. They include models that when plotted can be represented by straight lines, but also by curves and even curved surfaces.\n\nHere $\\beta$ enters the model equation as a multiplier of the $x_{i}.\n\n$$\ny_{ij} = \\beta_0 + \\beta_1\\,x_i + \\epsilon_{ij},\n$$\nAn example of a model non-linear in its parameters is\n\n$$\ny_{ij} = \\beta_0 + \\mathrm{e}^{\\beta_1\\,x_i} + \\epsilon_{ij},\n$$\nbecause $\\beta$ enters the model equation as a part of an exponent.\n\nWhether a model to be fitted is linear in its parameters or non-linear is crucial from the perspective of the computations needed to estimate the values of the parameters. From the interpretation perspective, and use, differences are rather small.\n:::\n\n::: callout-note\nIn linear models _dummy_ variables, consisting of zeros and ones, can be used to code\ntreatments levels. Thus, even though we rely on factors to describe treatments, these factors are converted into an encoding consistent with that used for regression, and the same computational procedure is used internally for regression and ANOVA.\n:::\n\n## Linear models in R\n\nBoth **ANOVA** and **regression** are described by **linear models** and have\nmuch more in common than what it looks at first sight.\n\nIn R little distinction is made between ANOVA and regression.\n\nThe function`lm()` can be used for both, whether the variables\nin the model are `factor`s or `numeric` vectors\ndetermines the coding (of the matrix used for describing the tests).\n\nThe function `aov()` uses `lm()` internally but differs in\nthat summary gives an ANOVA table, and in that it can also deal\ndirectly with hierarchical models (e.g. split-plot designs).\n\n## Regression examples\n\nI show here plots, but not not how one would do the data analysis in R.\n\n### Interpretation of the intercept\n\nWe generate some artificial data (it will be different each time you run the code below.)\n\nWhy will it be different? Try it?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- -20:80\ny <- x + 50 + rnorm(101, sd = 20)\nmy.data <- data.frame(x, y)\nsummary(my.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       x             y          \n Min.   :-20   Min.   :  9.244  \n 1st Qu.:  5   1st Qu.: 51.407  \n Median : 30   Median : 80.327  \n Mean   : 30   Mean   : 79.489  \n 3rd Qu.: 55   3rd Qu.:107.140  \n Max.   : 80   Max.   :163.511  \n```\n\n\n:::\n:::\n\n\nWe fit a linear regression (R's model formula `y ~ x` or its verbose equivalent `y ~ 1 + x`).\n\n$$\ny_{ij} = \\beta_0 + \\beta_1 x_i + \\epsilon_{ij},\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(my.data, aes(x, y)) +\n  geom_point() +\n  stat_poly_line(se = FALSE) +\n  stat_poly_eq(use_label(c(\"eq\")))\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-18-1.svg)\n:::\n:::\n\n\n### Residuals or deviations\n\nAll deviations from the fitted curve are assigned to variation in $y$, as an assumption of LM fitting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(my.data, aes(x, y)) +\n  geom_point() +\n  stat_poly_line(se = FALSE) +\n  stat_fit_deviations(colour = \"red\")\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-19-1.svg)\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(my.data, aes(x, y)) +\n  geom_hline(yintercept = 0) +\n  stat_fit_residuals(colour = \"red\")\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-20-1.svg)\n:::\n:::\n\n\n::: callout-caution\n# Assumption about $x$\n\n_It is assumed that the $x_i$ are measured without error._\n\nRegression provides a means of predicting $y$ from $x$, but not the reverse.\n\nFor example, this does not mean that there is no biological variation, but instead that the values of the independent variable are known. As with other assumptions, small deviations are rarely important.\n\nThis stems from the idea of dependent vs. independent variables, and cause and effect. We assume that all the variation that is observed in the $y_{ij}$ is caused by variation in the response to the known $x_{ij}$ or to the known $\\tau_i$.\n\n_This is a very frequent assumption, but methods do exist that do not involve this assumption._\n:::\n\n### Variability of the estimates\n\nA confidence band for the fitted model encloses the predicted relationship, in this case a straight line. It informs about the uncertainty about our estimate of the relationship for the whole population.\n\nWhile we use $\\beta_i$ for the values of the parameters in the population, we use $b_i$ for the estimates obtained by fitting the model to a sample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(my.data, aes(x, y)) +\n  geom_point() +\n  stat_poly_line() +\n  stat_fit_tb(label.x = \"left\",\n              tb.params = c(\"b[0]\" = 1, \"b[1]\" = 2),\n              tb.vars = c(\"Param.\" = 1, \"Estimate\" = 2,\n                          \"italic(s)[b[i]]\" = 3, \"italic(t)\" = 4,\n                          \"italic(P)\" = 5),\n              parse = TRUE)\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-21-1.svg)\n:::\n:::\n\n\n## Some code examples\n\nThe code chunks below show the steps for analysis of variance (ANOVA) applied to weights of three groups of plants. The example data are included in R. You can run the code directly in this web page (in the browser, no R installation needed). The same code, of course, runs in RStudio and R itself.\n\n```{webr-r}\nhelp(PlantGrowth)\n```\n\n```{webr-r}\nsummary(PlantGrowth)\n```\nA quick plot of the data.\n\n```{webr-r}\nboxplot(weight ~ group, data = PlantGrowth)\n```\n\nA bit more sophisticated plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(PlantGrowth, aes(group, weight)) +\n  geom_beeswarm() +\n  geom_boxplot(colour = \"red\", width = 0.33, fill = NA)\n```\n\n::: {.cell-output-display}\n![](linear-models-intro_files/figure-html/unnamed-chunk-22-1.svg)\n:::\n:::\n\n\n\n```{webr-r}\nfmPG <- lm(weight ~ group, data = PlantGrowth)\nfmPG\n```\n\n\n```{webr-r}\nplot(fmPG, which = 1)\nplot(fmPG, which = 2)\n# plot(fmPG, which = 3)\n# plot(fmPG, which = 4)\nplot(fmPG, which = 5)\n# plot(fmPG, which = 6)\n```\n\n\n```{webr-r}\nanova(fmPG)\n\n```\n\n\n**By editing the code in the chunks above, analyse the data set `chickwts`, also included in R.**\n",
    "supporting": [
      "linear-models-intro_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}